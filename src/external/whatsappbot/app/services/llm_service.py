"""
Gemini Service - Generates responses using LawYaar's legal research system.
This service acts as a bridge between WhatsApp messages and LawYaar's RAG system.
Maintains conversational context per user for a more natural chat experience.
"""

import logging
import asyncio
import sys
import os
import shelve
from pathlib import Path
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment variables from project root
env_path = Path(__file__).parent.parent.parent.parent.parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

# Add src directory to path to import LawYaar modules
src_path = str(Path(__file__).parent.parent.parent.parent.parent)
if src_path not in sys.path:
    sys.path.insert(0, src_path)

logger = logging.getLogger(__name__)

# Configure Gemini for conversational AI
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Model configuration for natural conversation
generation_config = {
    "temperature": 0.8,  # Slightly creative but still focused
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 2048,  # Concise for WhatsApp
}

# Initialize conversational model
conversation_model = genai.GenerativeModel(
    model_name="gemini-2.5-flash",
    generation_config=generation_config,
)

# Import LawYaar WhatsApp service
try:
    from whatsapp_legal_service import LawYaarWhatsAppService
    _lawyaar_service = None
    
    def get_lawyaar_service():
        """Get or create the LawYaar WhatsApp service singleton."""
        global _lawyaar_service
        if _lawyaar_service is None:
            _lawyaar_service = LawYaarWhatsAppService()
        return _lawyaar_service
    
    LAWYAAR_AVAILABLE = True
    logger.info("âœ… LawYaar legal research system loaded successfully")
except ImportError as e:
    logger.warning(f"âš ï¸ LawYaar system not available: {e}")
    LAWYAAR_AVAILABLE = False


# Chat history management (like example.py)
def check_if_chat_exists(wa_id):
    """Check if a chat session exists for this WhatsApp ID"""
    try:
        with shelve.open("chats_db") as chats_shelf:
            return chats_shelf.get(wa_id, None)
    except Exception as e:
        logger.error(f"Error checking chat existence: {e}")
        return None


def store_chat(wa_id, chat_history):
    """Store chat history for a WhatsApp ID"""
    try:
        with shelve.open("chats_db", writeback=True) as chats_shelf:
            chats_shelf[wa_id] = chat_history
    except Exception as e:
        logger.error(f"Error storing chat: {e}")


def _is_legal_query(message: str) -> str:
    """
    Classify a message into: LEGAL, CHITCHAT, or IRRELEVANT.
    
    Uses Gemini to quickly classify the message intent.
    
    Args:
        message: The user's message
        
    Returns:
        str: "LEGAL", "CHITCHAT", or "IRRELEVANT"
    """
    message_lower = message.lower().strip()
    
    # Quick keyword check for obvious greetings/chitchat (English + Urdu)
    chitchat_keywords = [
        # English greetings
        'hi', 'hello', 'hey', 'greetings', 'good morning', 'good afternoon', 'good evening',
        # Urdu/Arabic greetings
        'assalam', 'salam', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'ÙˆØ¹Ù„ÙŠÙƒÙ…', 'ÛÛŒÙ„Ùˆ', 'ÛØ§Ø¦ÛŒ',
        # Thanks/acknowledgments  
        'thanks', 'thank you', 'Ø´Ú©Ø±ÛŒÛ', 'shukriya', 'jazakallah',
        # Simple responses
        'ok', 'okay', 'Ù¹Ú¾ÛŒÚ©', 'Ø§Ú†Ú¾Ø§', 'theek', 'acha',
        # Farewells
        'bye', 'goodbye', 'Ø®Ø¯Ø§ Ø­Ø§ÙØ¸', 'allah hafiz', 'khuda hafiz',
        # Questions about bot
        'how are you', 'what is your name', 'who are you', 'Ú©ÙˆÙ† ÛÙˆ', 'Ù†Ø§Ù… Ú©ÛŒØ§'
    ]
    
    # If message is very short and matches chitchat, skip LLM call
    if len(message_lower) < 30 and any(keyword in message_lower for keyword in chitchat_keywords):
        logger.info(f"âœ… Quick chitchat detection: {message[:30]}")
        return "CHITCHAT"
    
    # For ambiguous cases, use LLM to classify
    try:
        from utils.call_llm import call_llm
        
        classification_prompt = f"""You are a message classifier for a Pakistani legal assistant chatbot.

USER MESSAGE: "{message}"

TASK: Classify this message into ONE category:

A) CHITCHAT - Greetings, small talk, acknowledgments, questions about the bot
   Examples:
   - "Hi", "Hello", "Assalam o alaikum", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…"
   - "How are you?", "What's your name?", "Who are you?"
   - "Thanks", "Thank you", "Ø´Ú©Ø±ÛŒÛ", "OK", "Okay", "Ù¹Ú¾ÛŒÚ© ÛÛ’"
   - "Bye", "Goodbye", "Ø®Ø¯Ø§ Ø­Ø§ÙØ¸", "Allah hafiz"
   - Any greeting or social pleasantry

B) LEGAL - Questions about Pakistani law, legal rights, procedures, cases
   Examples:
   - "What are grounds for eviction?"
   - "Can I get bail in a murder case?"
   - "What are tenant rights in Pakistan?"
   - "How to file a petition?"
   - ANY question related to law, courts, legal rights, procedures
   
C) IRRELEVANT - Topics unrelated to law OR the bot
   Examples:
   - "What's the weather?", "Tell me a joke", "Recipe for biryani"
   - Math problems, sports scores, movie recommendations
   - General knowledge NOT related to law

IMPORTANT RULES:
1. If message is a greeting (hi, hello, salam, etc.) â†’ CHITCHAT
2. If message asks about law/legal matters â†’ LEGAL
3. Only use IRRELEVANT for topics completely outside law and greetings
4. When in doubt between CHITCHAT and LEGAL â†’ choose CHITCHAT for greetings

Respond with ONLY one word: "CHITCHAT", "LEGAL", or "IRRELEVANT"

CLASSIFICATION:"""
        
        result = call_llm(classification_prompt).strip().upper()
        
        # Extract classification (prioritize CHITCHAT for greetings)
        if "CHITCHAT" in result:
            classification = "CHITCHAT"
        elif "LEGAL" in result:
            classification = "LEGAL"
        elif "IRRELEVANT" in result:
            classification = "IRRELEVANT"
        else:
            # Default to LEGAL to be safe
            classification = "LEGAL"
        
        logger.info(f"ğŸ¤– LLM classification: {classification} - Message: {message[:50]}")
        return classification
        
    except Exception as e:
        logger.error(f"Error classifying message: {e}")
        # Default to LEGAL to be safe (better to over-search than miss queries)
        return "LEGAL"


def _handle_chitchat(message: str, wa_id: str, name: str) -> str:
    """
    Generate a friendly conversational response for non-legal messages.
    Automatically detects language and responds accordingly.
    
    Args:
        message: The user's message
        wa_id: WhatsApp ID
        name: User's name
        
    Returns:
        str: Friendly conversational response in matching language
    """
    try:
        # Detect language of user's message
        detected_lang = _detect_language(message)
        logger.info(f"ğŸ’¬ Chitchat detected in {'Urdu' if detected_lang == 'ur' else 'English'}")
        
        # Get chat history for context
        chat_history = check_if_chat_exists(wa_id)
        
        from utils.call_llm import call_llm
        
        # Language-specific prompt
        if detected_lang == 'ur':
            chitchat_prompt = f"""You are a friendly Pakistani legal assistant chatbot on WhatsApp named "LawYaar".

USER: {name}
MESSAGE: {message}

Generate a warm, brief, conversational response IN URDU (2-3 sentences max). 

Guidelines:
- Respond in URDU script (Ø§Ø±Ø¯Ùˆ)
- Be friendly and professional
- If greeting, greet back and offer help with legal questions
- If thanks, acknowledge and offer further assistance
- Keep it SHORT (this is WhatsApp)
- Use emojis sparingly ğŸ˜Š

URDU RESPONSE:"""
        else:
            chitchat_prompt = f"""You are a friendly Pakistani legal assistant chatbot on WhatsApp named "LawYaar".

USER: {name}
MESSAGE: {message}

Generate a warm, brief, conversational response IN ENGLISH (2-3 sentences max). 

Guidelines:
- Respond in ENGLISH
- Be friendly and professional
- If greeting, greet back and offer help with legal questions
- If thanks, acknowledge and offer further assistance
- Keep it SHORT (this is WhatsApp)
- Use emojis sparingly ğŸ˜Š

ENGLISH RESPONSE:"""
        
        chitchat_response = call_llm(chitchat_prompt).strip()
        
        # Store in chat history
        new_history = chat_history if chat_history else []
        new_history.append({"role": "user", "parts": [message]})
        new_history.append({"role": "model", "parts": [chitchat_response]})
        store_chat(wa_id, new_history)
        
        logger.info(f"âœ… Chitchat response generated for {name} in {'Urdu' if detected_lang == 'ur' else 'English'}")
        return chitchat_response
        
    except Exception as e:
        logger.error(f"Error generating chitchat response: {e}")
        # Fallback responses
        detected_lang = _detect_language(message)
        if detected_lang == 'ur':
            return "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…! Ù…ÛŒÚº LawYaar ÛÙˆÚºØŒ Ø¢Ù¾ Ú©Ø§ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø¹Ø§ÙˆÙ† ğŸ˜Š Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ú©ÛŒØ³Û’ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªØ§ ÛÙˆÚºØŸ"
        return "Hello! I'm LawYaar, your legal assistant ğŸ˜Š How can I help you with legal questions today?"


def _handle_irrelevant(message: str, wa_id: str, name: str) -> str:
    """
    Politely decline irrelevant (non-legal) queries.
    
    Args:
        message: The user's message
        wa_id: WhatsApp ID
        name: User's name
        
    Returns:
        str: Polite rejection message
    """
    # Detect language for appropriate response
    detected_lang = _detect_language(message)
    
    # Store in chat history
    try:
        chat_history = check_if_chat_exists(wa_id)
        new_history = chat_history if chat_history else []
    except:
        new_history = []
    
    if detected_lang == 'ur':
        response = (
            "Ù…Ø¹Ø°Ø±Øª! ğŸ˜Š Ù…ÛŒÚº LawYaar ÛÙˆÚº - Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ÛŒÚº Ù…ÛØ§Ø±Øª Ø±Ú©Ú¾Ù†Û’ ÙˆØ§Ù„Ø§ \n"
            "Ù…ÛŒÚº ØµØ±Ù Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯Û’ Ø³Ú©ØªØ§ ÛÙˆÚº Ø¬ÛŒØ³Û’:\n"
            "â€¢ Ø¶Ù…Ø§Ù†Øª Ø§ÙˆØ± Ø³Ø²Ø§\n"
            "â€¢ Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©Û’ ÙÛŒØµÙ„Û’\n"
            "â€¢ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø­Ù‚ÙˆÙ‚ Ø§ÙˆØ± Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø±\n\n"
            "Ø¨Ø±Ø§Û Ú©Ø±Ù… Ú©ÙˆØ¦ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ†Ú¾ÛŒÚº! âš–ï¸"
        )
    else:
        response = (
            "I apologize! ğŸ˜Š I'm LawYaar - a legal assistant specializing in Pakistani law.\n\n"
            "I can only help with legal questions such as:\n"
            "â€¢ Bail and sentencing matters\n"
            "â€¢ Supreme Court case law\n"
            "â€¢ Legal rights and procedures\n\n"
            "Please ask me a legal question! âš–ï¸"
        )
    
    # Store in chat history
    try:
        new_history.append({"role": "user", "parts": [message]})
        new_history.append({"role": "model", "parts": [response]})
        store_chat(wa_id, new_history)
    except Exception as e:
        logger.error(f"Error storing irrelevant query history: {e}")
    
    logger.info(f"ğŸš« Irrelevant query rejected for {name}: {message[:50]}")
    return response


def get_legal_context(message, wa_id, name):
    """
    Get relevant legal context from LawYaar RAG system.
    
    Args:
        message: User's question
        wa_id: WhatsApp ID
        name: User's name
        
    Returns:
        str: Relevant legal context or empty string
    """
    if not LAWYAAR_AVAILABLE:
        return ""
    
    try:
        service = get_lawyaar_service()
        
        # Run async function in a thread-safe way
        try:
            # Check if we're in an async context
            try:
                asyncio.get_running_loop()
                # We're in an async context, use thread executor
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run,
                        service.generate_legal_response(message, wa_id, name)
                    )
                    context = future.result(timeout=120)
            except RuntimeError:
                # No running loop, safe to use asyncio.run directly
                context = asyncio.run(
                    service.generate_legal_response(message, wa_id, name)
                )
        except Exception as e:
            logger.error(f"Error running async context retrieval: {e}")
            raise
        
        return context
    except Exception as e:
        logger.error(f"Error getting legal context: {e}")
        return ""


def generate_response(message, wa_id, name, message_source='text'):
    """
    Generate a HYBRID response: Friendly summary + Full legal research + PDF links.
    
    Uses LawYaar's multi-agent legal research pipeline (same as CLI) but wraps it
    in a conversational, easy-to-understand format for WhatsApp users.
    
    Architecture:
    1. Check if message is a legal query (filter greetings/chitchat)
    2. Run full legal research pipeline (Classification â†’ Retrieval â†’ Pruning â†’ Reading â†’ Aggregation)
    3. For TEXT queries: Generate and send PDF immediately
    4. For VOICE queries: Send voice summary + PDF offer
    
    Args:
        message (str): The user's legal query
        wa_id (str): WhatsApp ID of the user
        name (str): Name of the user
        message_source (str): 'text' or 'voice' - determines response format
        
    Returns:
        For voice: dict with voice_summary and research_data
        For text: dict with pdf_path for immediate PDF sending
    """
    try:
        logger.info(f"ğŸ” Processing {'TEXT' if message_source == 'text' else 'VOICE'} query for {name}: {message[:100]}...")
        
        if not LAWYAAR_AVAILABLE:
            logger.error("âŒ LawYaar legal research system not available")
            return ("I apologize, but the legal research system is currently unavailable. "
                   "Please try again later.")
        
        # âœ¨ INTELLIGENT ROUTING WITH PDF REQUEST PRIORITY
        
        # STEP 0: CHECK FOR PDF REQUEST FIRST (before classification)
        # This prevents "yes", "ok" from being classified as chitchat when user is responding to PDF offer
        chat_history = check_if_chat_exists(wa_id)
        if chat_history and len(chat_history) > 0:
            last_bot_message = None
            for msg in reversed(chat_history):
                if msg.get('role') == 'model' and 'research_data' in msg:
                    last_bot_message = msg
                    break
            
            # Check if there's a PENDING PDF offer
            has_pending_pdf = (last_bot_message and 
                             last_bot_message.get('research_data', {}).get('type') == 'pending_pdf_request')
            
            # If pending PDF exists AND message is short (likely a response), check for affirmative FIRST
            message_word_count = len(message.split())
            is_short_response = message_word_count <= 5
            
            if has_pending_pdf and is_short_response and _is_pdf_request(message):
                logger.info(f"ğŸ“„ PDF request detected BEFORE classification (short affirmative after legal query)")
                research_data = last_bot_message.get('research_data', {})
                
                # Get language before PDF generation
                detected_lang = research_data.get('detected_language', 'en')
                
                # Generate PDF
                pdf_path = generate_pdf_report(wa_id, name, research_data)
                
                # âœ… CLEAR PENDING PDF STATE - Mark as fulfilled
                try:
                    research_data['type'] = 'pdf_fulfilled'  # Change state
                    # Update chat history to mark PDF as sent
                    for msg in reversed(chat_history):
                        if msg.get('role') == 'model' and 'research_data' in msg:
                            msg['research_data']['type'] = 'pdf_fulfilled'
                            break
                    store_chat(wa_id, chat_history)
                    logger.info("âœ… Marked PDF state as fulfilled")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not update PDF state: {e}")
                
                if pdf_path:
                    if detected_lang == 'ur':
                        return {
                            "type": "pdf_response",
                            "pdf_path": pdf_path,
                            "message": "Ø¨ÛØªØ±ÛŒÙ†! Ù…ÛŒÚº Ø¢Ù¾ Ú©Û’ Ù„ÛŒÛ’ ØªÙØµÛŒÙ„ÛŒ Ø±Ù¾ÙˆØ±Ù¹ ØªÛŒØ§Ø± Ú©Ø± Ø±ÛØ§ ÛÙˆÚºÛ” ÛŒÛ Ø±Ù¾ÙˆØ±Ù¹ ØªÙ…Ø§Ù… Ú©ÛŒØ³Ø² Ú©ÛŒ ØªÙØµÛŒÙ„Ø§ØªØŒ Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø§ÙˆØ± Ù„Ù†Ú©Ø³ Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÛ’Û” ğŸ“„"
                        }
                    else:
                        return {
                            "type": "pdf_response",
                            "pdf_path": pdf_path,
                            "message": "Great! I'm preparing your detailed report with all case citations and links. ğŸ“„"
                        }
                else:
                    if detected_lang == 'ur':
                        return "Ù…Ø¹Ø°Ø±Øª! PDF Ø±Ù¾ÙˆØ±Ù¹ Ø¨Ù†Ø§Ù†Û’ Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ ÛÙˆØ¦ÛŒÛ” Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø¯ÙˆØ¨Ø§Ø±Û Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚºÛ”"
                    else:
                        return "I apologize! There was an error generating the PDF report. Please try again."
        
        # STEP 1: Classify the message (LEGAL, CHITCHAT, or IRRELEVANT)
        message_type = _is_legal_query(message)
        logger.info(f"ğŸ“Š Message classified as: {message_type}")
        
        # STEP 2: Handle NON-LEGAL messages immediately (don't check for PDF)
        # âœ… IMPORTANT: Non-legal messages also invalidate any pending PDF offers
        if message_type == "CHITCHAT":
            logger.info(f"ğŸ’¬ Chitchat detected: {message[:50]}... Responding conversationally")
            
            # Invalidate any pending PDF offer
            chat_history = check_if_chat_exists(wa_id)
            if chat_history and len(chat_history) > 0:
                try:
                    for msg in reversed(chat_history):
                        if msg.get('role') == 'model' and 'research_data' in msg:
                            if msg['research_data'].get('type') == 'pending_pdf_request':
                                msg['research_data']['type'] = 'pdf_expired'
                                logger.info("ğŸ”„ Invalidated pending PDF offer - user sent chitchat")
                            break
                    store_chat(wa_id, chat_history)
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not invalidate PDF state: {e}")
            
            return _handle_chitchat(message, wa_id, name)
            
        elif message_type == "IRRELEVANT":
            logger.info(f"ğŸš« Irrelevant query detected: {message[:50]}... Politely declining")
            
            # Invalidate any pending PDF offer
            chat_history = check_if_chat_exists(wa_id)
            if chat_history and len(chat_history) > 0:
                try:
                    for msg in reversed(chat_history):
                        if msg.get('role') == 'model' and 'research_data' in msg:
                            if msg['research_data'].get('type') == 'pending_pdf_request':
                                msg['research_data']['type'] = 'pdf_expired'
                                logger.info("ğŸ”„ Invalidated pending PDF offer - user sent irrelevant query")
                            break
                    store_chat(wa_id, chat_history)
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not invalidate PDF state: {e}")
            
            return _handle_irrelevant(message, wa_id, name)
        
        
        # STEP 4: Process as NEW LEGAL QUERY (message_type == "LEGAL")
        # âœ… IMPORTANT: Automatically invalidate any old pending PDF offers
        # User has moved on to a new query, so old offer is no longer relevant
        if chat_history and len(chat_history) > 0:
            try:
                for msg in reversed(chat_history):
                    if msg.get('role') == 'model' and 'research_data' in msg:
                        old_state = msg['research_data'].get('type')
                        if old_state == 'pending_pdf_request':
                            msg['research_data']['type'] = 'pdf_expired'  # Mark as expired
                            logger.info("ğŸ”„ Invalidated old pending PDF offer - user moved to new query")
                        break
                store_chat(wa_id, chat_history)
            except Exception as e:
                logger.warning(f"âš ï¸ Could not invalidate old PDF state: {e}")
        
        logger.info(f"âš–ï¸ Processing new legal query: {message[:50]}...")
        
        # Run full legal research pipeline with metadata
        service = get_lawyaar_service()
        
        # Use asyncio.run() which creates a fresh event loop - handles thread safety
        import nest_asyncio
        try:
            # Try to enable nested event loops (needed for some environments)
            nest_asyncio.apply()
        except:
            pass  # nest_asyncio not available, that's ok
        
        # Run async function in a thread-safe way
        try:
            # Check if we're in an async context
            try:
                asyncio.get_running_loop()
                # We're in an async context, use thread executor
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run,
                        service.generate_legal_response(message, wa_id, name, return_metadata=True)
                    )
                    research_data = future.result(timeout=180)
            except RuntimeError:
                # No running loop, safe to use asyncio.run directly
                research_data = asyncio.run(
                    service.generate_legal_response(message, wa_id, name, return_metadata=True)
                )
        except Exception as e:
            logger.error(f"Error running async research: {e}")
            raise
        
        # Safety check: ensure research_data is a dict
        if not isinstance(research_data, dict):
            logger.error(f"âŒ Expected dict from generate_legal_response, got {type(research_data)}: {research_data}")
            # If it's a string error message, return it
            if isinstance(research_data, str):
                return research_data
            # Otherwise return generic error
            return "I apologize, but I encountered an error while researching your legal question. Please try again."
        
        # Extract research components
        full_legal_response = research_data.get("full_legal_response", "")
        pdf_links = research_data.get("pdf_links", [])
        doc_count = research_data.get("document_count", 0)
        detected_language = research_data.get("detected_language", "en")
        
        if not full_legal_response:
            logger.warning("Empty legal research response")
            return "I apologize, but I couldn't generate a response to your legal query. Please try rephrasing."
        
        # Check if no relevant cases were found
        no_cases_found = (
            doc_count == 0 or
            "could not find any relevant legal cases" in full_legal_response.lower() or
            "apologize" in full_legal_response.lower() and "could not find" in full_legal_response.lower()
        )
        
        # Create VOICE-OPTIMIZED dense summary (for illiterate users - no citations)
        from utils.call_llm import call_llm
        
        voice_summary_prompt = f"""You are a friendly legal assistant helping an illiterate user via WhatsApp voice message.

YOUR TASK: Create a DENSE, COMPREHENSIVE VOICE SUMMARY that:
- DIRECTLY ANSWERS the user's legal question in simple, spoken language
- Includes ALL important legal information from the research
- Explains the legal principles, procedures, and rights clearly
- Uses conversational tone suitable for audio (as if talking to a friend)
- NO case numbers, NO citations, NO metadata (user can't see/read them in voice)
- NO bullet points, NO special formatting (this is for AUDIO, not text!)
- Keep it focused but comprehensive (400-500 words for voice)
- Use examples and analogies when helpful
- Structure as natural paragraphs with clear flow
- In {'Urdu' if detected_language == 'ur' else 'Sindhi' if detected_language == 'sd' else 'Balochi' if detected_language == 'bl' else 'English'}

USER'S QUESTION: {message}

DETAILED LEGAL RESEARCH WITH ALL FINDINGS:
{full_legal_response}

IMPORTANT: Synthesize ALL the key legal information into a natural spoken explanation. Imagine you're explaining to someone who cannot read. Be thorough but natural. Remember: this will be converted to AUDIO, so write as you would SPEAK, not as you would WRITE.

VOICE SUMMARY:"""
        
        try:
            voice_summary = call_llm(voice_summary_prompt).strip()
        except Exception as e:
            logger.error(f"âš ï¸ LLM API error generating voice summary: {e}")
            # Fallback: Use first two paragraphs of legal response
            paragraphs = full_legal_response.split('\n\n')
            if len(paragraphs) >= 2:
                voice_summary = '\n\n'.join(paragraphs[:2])
            elif paragraphs:
                voice_summary = paragraphs[0]
            else:
                voice_summary = "Here's what I found from the legal research:"
        
        # âœ… DIFFERENT HANDLING FOR TEXT vs VOICE
        if message_source == 'text':
            # For TEXT queries: Send SUMMARY + PDF immediately
            logger.info(f"ğŸ“„ TEXT query detected - generating executive summary + PDF")
            
            # Create EXECUTIVE SUMMARY (NO metadata - just legal findings)
            text_summary_prompt = f"""You are a professional legal assistant providing an executive summary to a literate user via WhatsApp.

YOUR TASK: Create a DENSE, EXECUTIVE SUMMARY that:
- DIRECTLY ANSWERS the user's legal question with the key findings
- Focuses ONLY on legal principles, rights, procedures, and outcomes
- Synthesizes findings from multiple cases into coherent principles
- NO case names, NO judge names, NO dates, NO citation numbers
- NO metadata - save all that for the detailed PDF
- Uses bullet points for clarity
- Professional but accessible language
- In {'Urdu' if detected_language == 'ur' else 'Sindhi' if detected_language == 'sd' else 'Balochi' if detected_language == 'bl' else 'English'}
- Keep it concise and actionable (200-300 words)

USER'S QUESTION: {message}

DETAILED LEGAL RESEARCH WITH ALL FINDINGS:
{full_legal_response}

DOCUMENT COUNT: {doc_count} relevant cases analyzed

CRITICAL RULES:
- Extract ONLY the legal principles and findings
- DO NOT mention case names (e.g., "Ali v. Hassan") 
- DO NOT mention judges, courts, or dates
- DO NOT include citations or reference numbers
- Focus on WHAT the law says, not WHERE it comes from
- End with a brief note that detailed citations are in the attached PDF

EXECUTIVE SUMMARY:"""
            
            try:
                text_summary = call_llm(text_summary_prompt).strip()
                logger.info(f"âœ… Generated executive summary: {len(text_summary)} chars")
            except Exception as e:
                logger.error(f"âš ï¸ LLM API error generating text summary: {e}")
                # Fallback: Use voice summary
                text_summary = voice_summary
            
            # Store research data for chat history
            research_context = {
                "type": "pdf_fulfilled",  # Mark as already fulfilled
                "query": message,
                "full_legal_response": full_legal_response,
                "pdf_links": pdf_links,
                "doc_count": doc_count,
                "detected_language": detected_language,
                "text_summary": text_summary
            }
            
            # Generate PDF immediately
            pdf_path = generate_pdf_report(wa_id, name, research_context)
            
            # Store in chat history
            try:
                chat_history = check_if_chat_exists(wa_id)
                if not chat_history:
                    chat_history = []
                
                chat_history.append({"role": "user", "parts": [message]})
                chat_history.append({
                    "role": "model", 
                    "parts": [text_summary],
                    "research_data": research_context
                })
                store_chat(wa_id, chat_history)
            except Exception as e:
                logger.error(f"Error storing chat history: {e}")
            
            # Return BOTH summary AND PDF
            if pdf_path:
                if detected_language == 'ur':
                    pdf_message = f"ğŸ“„ ÛŒÛØ§Úº {doc_count} Ù…ØªØ¹Ù„Ù‚Û Ú©ÛŒØ³Ø² Ú©Û’ Ø³Ø§ØªÚ¾ ØªÙØµÛŒÙ„ÛŒ PDF Ø±Ù¾ÙˆØ±Ù¹ ÛÛ’Û”"
                elif detected_language == 'sd':
                    pdf_message = f"ğŸ“„ Ù‡ØªÙŠ {doc_count} Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ ÚªÙŠØ³Ø² Ø³Ø§Ù† Ú¯Ú ØªÙØµÙŠÙ„ÙŠ PDF Ø±Ù¾ÙˆØ±Ù½ Ø¢Ù‡ÙŠ."
                elif detected_language == 'bl':
                    pdf_message = f"ğŸ“„ Ø§ØªÙŠ {doc_count} Ù…ØªØ¹Ù„Ù‚Û Ú©ÛŒØ³Ø² Ú©Û’ Ø³Ø§ØªÚ¾ ØªÙØµÛŒÙ„ÛŒ PDF Ø±Ù¾ÙˆØ±Ù¹ ÛÛ’Û”"
                else:
                    pdf_message = f"ğŸ“„ Here's the detailed PDF report with {doc_count} relevant cases."
                return {
                    "type": "text_with_pdf",
                    "text_summary": text_summary,
                    "pdf_path": pdf_path,
                    "pdf_message": pdf_message
                }
            else:
                # PDF generation failed - send text summary only
                logger.error("âŒ PDF generation failed for text query - sending summary only")
                if detected_language == 'ur':
                    return text_summary + f"\n\nâš ï¸ Ù…Ø¹Ø°Ø±Øª! PDF Ø¨Ù†Ø§Ù†Û’ Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ ÛÙˆØ¦ÛŒÛ”"
                else:
                    return text_summary + f"\n\nâš ï¸ Sorry! PDF generation failed."
        
        else:
            # For VOICE queries: Send voice summary + PDF OFFER (existing flow)
            logger.info(f"ğŸ¤ VOICE query detected - sending summary with PDF offer")
            
            # Add PDF offer at the end of voice summary
            if detected_language == 'ur':
                pdf_offer = f"\n\nØ§Ú¯Ø± Ø¢Ù¾ Ù…Ú©Ù…Ù„ ØªÙØµÛŒÙ„ÛŒ Ø±Ù¾ÙˆØ±Ù¹ Ú†Ø§ÛØªÛ’ ÛÛŒÚº Ø¬Ø³ Ù…ÛŒÚº ØªÙ…Ø§Ù… Ú©ÛŒØ³Ø² Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª Ø§ÙˆØ± Ù„Ù†Ú©Ø³ ÛÙˆÚºØŒ ØªÙˆ Ø¨Ø±Ø§Û Ú©Ø±Ù… 'ÛØ§Úº' ÛŒØ§ 'Ø¬ÛŒ' Ø¨Ú¾ÛŒØ¬ÛŒÚºÛ” Ù…ÛŒÚº Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒÚ© ØªÙØµÛŒÙ„ÛŒ PDF Ø¯Ø³ØªØ§ÙˆÛŒØ² Ø¨Ú¾ÛŒØ¬ Ø¯ÙˆÚº Ú¯Ø§Û”"
            elif detected_language == 'sd':
                pdf_offer = f"\n\nØ¬ÙŠÚªÚÙ‡Ù† ØªÙˆÙ‡Ø§Ù† Ø³Ú€Ù†ÙŠ ÚªÙŠØ³Ø² Ø¬ÙŠ ØªÙØµÙŠÙ„Ù† Û½ Ù„Ù†ÚªØ³ Ø³Ø§Ù† Ú¯Ú Ù…ÚªÙ…Ù„ ØªÙØµÙŠÙ„ÙŠ Ø±Ù¾ÙˆØ±Ù½ Ú†Ø§Ù‡ÙŠÙˆ Ù¿Ø§ØŒ ØªÙ‡ Ù…Ù‡Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ 'Ù‡Ø§' ÙŠØ§ 'Ø¬ÙŠ' Ù…ÙˆÚªÙ„ÙŠÙˆ. Ø¢Ø¦ÙˆÙ† ØªÙˆÙ‡Ø§Ù† Ú©ÙŠ Ù‡Úª Ø¬Ø§Ù…Ø¹ PDF Ø¯Ø³ØªØ§ÙˆÙŠØ² Ù…ÙˆÚªÙ„ÙŠÙ†Ø¯Ø³."
            elif detected_language == 'bl':
                pdf_offer = f"\n\nØ§Ú¯Ø± ØªÛØ§Ù† Ø³Ø§Ø± Ú©ÙŠØ³Ø§Úº Ø¯ÛŒ ØªÙØµÛŒÙ„Ø§Úº ØªÛ’ Ù„Ù†Ú©Ø³ Ù†Ø§Ù„ Ù…Ù„ Ú©Ø± Ù…Ú©Ù…Ù„ ØªÙØµÛŒÙ„ÛŒ Ø±Ù¾ÙˆØ±Ù¹ Ú†Ø§ÛØªÛ’ ÛÙˆØŒ ØªÙˆ Ø¨Ø±Ø§Ø¦Û’ Ù…ÛØ±Ø¨Ø§Ù†ÛŒ 'ÛØ§Úº' ÛŒØ§ 'Ø¬ÛŒ' Ø¨Ú¾ÛŒØ¬ÙˆÛ” Ù…ÛŒÚº ØªÛØ§Ù† Ú©Ùˆ Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ PDF Ø¯Ø³ØªØ§ÙˆÛŒØ² Ø¨Ú¾ÛŒØ¬ÙˆÚº Ú¯Ø§Û”"
            else:
                pdf_offer = f"\n\nIf you'd like a detailed report with all case citations and links, please reply with 'yes' or 'haan'. I'll send you a comprehensive PDF document."
            
            voice_summary_with_offer = voice_summary + pdf_offer
            
            # Store research data for later PDF generation
            research_context = {
                "type": "pending_pdf_request",
                "query": message,
                "full_legal_response": full_legal_response,
                "pdf_links": pdf_links,
                "doc_count": doc_count,
                "detected_language": detected_language,
                "voice_summary": voice_summary
            }
            
            # Store in chat history
            try:
                chat_history = check_if_chat_exists(wa_id)
                if not chat_history:
                    chat_history = []
                
                chat_history.append({"role": "user", "parts": [message]})
                chat_history.append({
                    "role": "model", 
                    "parts": [voice_summary],
                    "research_data": research_context
                })
                store_chat(wa_id, chat_history)
            except Exception as e:
                logger.error(f"Error storing chat history: {e}")
            
            # Return voice response with PDF prep data
            logger.info(f"âœ… Voice-optimized summary complete: {len(voice_summary)} characters")
            
            return {
                "type": "voice_with_pdf_prep",
                "voice_summary": voice_summary,
                "research_data": research_context,
                "detected_language": detected_language
            }
        
    except Exception as e:
        logger.error(f"âŒ Critical error in generate_response: {e}", exc_info=True)
        # Never expose internal errors to user
        try:
            detected_lang = _detect_language(message) if message else 'en'
        except:
            detected_lang = 'en'
        
        if detected_lang == 'ur':
            return (
                "Ù…Ø¹Ø°Ø±Øª!  Ù…Ø¬Ú¾Û’ Ø¢Ù¾ Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙ†Û’ Ù…ÛŒÚº Ø¯Ø´ÙˆØ§Ø±ÛŒ ÛÙˆ Ø±ÛÛŒ ÛÛ’Û”\n\n"
                "Ø¨Ø±Ø§Û Ú©Ø±Ù…:\n"
                "â€¢ Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ Ø¯ÙˆØ¨Ø§Ø±Û Ù„Ú©Ú¾ÛŒÚº\n"
                "â€¢ ÛŒØ§ Ú©Ú†Ú¾ Ø¯ÛŒØ± Ø¨Ø¹Ø¯ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº\n\n"
                "Ø´Ú©Ø±ÛŒÛ! "
            )
        elif detected_lang == 'sd':
            return (
                "Ù…Ø¹Ø§ÙÙŠ ÚªØ¬Ùˆ!  Ù…ÙˆÙ† ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ø³ÙˆØ§Ù„ Ø¬Ùˆ Ø¬ÙˆØ§Ø¨ ÚÙŠÚ» Û¾ ØªÚªÙ„ÙŠÙ Ù¿ÙŠ Ø±Ù‡ÙŠ Ø¢Ù‡ÙŠ.\n\n"
                "Ù…Ù‡Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ:\n"
                "â€¢ Ù¾Ù†Ú¾Ù†Ø¬Ùˆ Ø³ÙˆØ§Ù„ Ù»ÙŠÙ‡Ø± Ù„Ú©Ùˆ\n"
                "â€¢ ÙŠØ§ ÚªØ¬Ú¾ Ø¯ÙŠØ± Ø¨Ø¹Ø¯ ÚªÙˆØ´Ø´ ÚªØ±ÙŠÙˆ\n\n"
                "Ù…Ù‡Ø±Ø¨Ø§Ù†ÙŠ! "
            )
        elif detected_lang == 'bl':
            return (
                "Ù…Ø¹Ø§ÙÛŒ!  Ù…Ø¬Ú¾Û’ Ø¢Ù¾ Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙ†Û’ Ù…ÛŒÚº Ø¯Ø´ÙˆØ§Ø±ÛŒ ÛÙˆ Ø±ÛÛŒ ÛÛ’Û”\n\n"
                "Ø¨Ø±Ø§Ø¦Û’ Ù…ÛØ±Ø¨Ø§Ù†ÛŒ:\n"
                "â€¢ Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ Ø¯ÙˆØ¨Ø§Ø±Û Ù„Ú©Ú¾ÛŒÚº\n"
                "â€¢ ÛŒØ§ Ú©Ú†Ú¾ Ø¯ÛŒØ± Ø¨Ø¹Ø¯ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº\n\n"
                "Ø´Ú©Ø±ÛŒÛ!"
            )
        return (
            "I apologize! I'm having trouble processing your question.\n\n"
            "Please try:\n"
            "â€¢ Rephrasing your question\n"
            "â€¢ Asking again in a few moments\n\n"
            "Thank you for your patience! ğŸ™"
        )


def _detect_language(text: str) -> str:
    """
    Detect if text is in Urdu, Sindhi, Balochi, or English using LLM for intelligent detection.

    Args:
        text: Input text to detect language

    Returns:
        str: 'ur' for Urdu, 'sd' for Sindhi, 'bl' for Balochi, 'en' for English
    """
    # Use LLM for intelligent detection
    try:
        from utils.call_llm import call_llm

        detection_prompt = f"""Analyze this text and determine the primary language being used.

TEXT TO ANALYZE: "{text}"

LANGUAGE CLASSIFICATION TASK:
- If the text is primarily in ENGLISH, respond with "ENGLISH"
- If the text is primarily in URDU (even if mixed with English), respond with "URDU"
- If the text is primarily in SINDHI (even if mixed with English), respond with "SINDHI"
- If the text is primarily in BALOCHI (even if mixed with English), respond with "BALOCHI"

CONSIDER:
1. Script: Urdu/Sindhi/Balochi use Arabic script, English uses Latin
2. Context: Legal questions about Pakistan often indicate Urdu unless specified otherwise
3. Keywords: Look for language-specific terms, place names, or cultural references
4. Mixing: If text has both scripts, prioritize the non-English script
5. Linguistic patterns: Consider grammar, vocabulary, and sentence structure unique to each language

EXAMPLES:
- "What are tenant rights in Pakistan?" â†’ ENGLISH
- "Ú©ÛŒØ§ Ú©Ø±Ø§ÛŒÛ Ø¯Ø§Ø± Ú©Û’ Ø­Ù‚ÙˆÙ‚ Ú©ÛŒØ§ ÛÛŒÚºØŸ" â†’ URDU
- "ÚªØ±Ø§Ú†ÙŠ Û¾ ÚªØ±Ø§Ø¦ÙŠØ¯Ø§Ø± Ø¬Ø§ Ø­Ù‚ Ú‡Ø§ Ø¢Ù‡Ù†ØŸ" â†’ SINDHI
- "Ú©ÙØ±Ø§ÛŒÙØ¯Ø§Ø±Ø§Úº Ú©Û’ Ú©ÙÛ’ Ø­ÙÙ‚ÙÙˆÙ‚ Ø¡ÙÙ†ØªØŸ" â†’ BALOCHI
- "Tell me about divorce laws in Urdu" â†’ URDU (explicitly requested)
- "Ø³Ù†ÚŒÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø¨Ø§Ø¨Øª Ø¨ØªØ§Ø¤" â†’ SINDHI
- "Ú©ÙØ±Ø§ÛŒÙØ¯Ø§Ø±Ø§Úº Ú©Û’ Ú©ÙÛ’ Ø­ÙÙ‚ÙÙˆÙ‚ Ø¡ÙÙ†ØªØŸ"â†’ BALOCHI
- "Property dispute in Karachi" â†’ ENGLISH (but context suggests Urdu response might be preferred)
- "Ù…ÛŒØ±Ø§ Ú¯Ú¾Ø± Ú†Ú¾ÛŒÙ† Ù„ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’" â†’ URDU
- "Ù…ÙˆÙ† Ú©ÙŠ Ú¯Ù‡Ø± Ú©Ø³ÙŠ Ú†ÙˆØ±ÙŠ ÚªØ±ÙŠ ÙˆØ±ØªÙˆ" â†’ SINDHI
- "Ù…ÙˆØ± Ú¯ÙØ± Ú†ÙˆØ±ÛŒ ÚªÙØª Ú¯Ø¦ÛŒ" â†’ BALOCHI

Respond with ONLY ONE WORD: "ENGLISH", "URDU", "SINDHI", or "BALOCHI"

DETECTED LANGUAGE:"""

        result = call_llm(detection_prompt).strip().upper()

        # Map LLM response to our codes
        if "URDU" in result:
            return 'ur'
        elif "SINDHI" in result:
            return 'sd'
        elif "BALOCHI" in result:
            return 'bl'
        elif "ENGLISH" in result:
            return 'en'
        else:
            # Fallback to script-based detection
            logger.warning(f"LLM returned unclear result: {result}, falling back to script detection")
    except Exception as e:
        logger.error(f"LLM language detection failed: {e}, falling back to script detection")

    # Fallback: Count Urdu/Arabic script characters
    urdu_arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or '\u0750' <= char <= '\u077F')

    # If more than 20% Urdu/Arabic characters, consider it Urdu
    if len(text) > 0 and urdu_arabic_chars > len(text) * 0.2:
        return 'ur'
    return 'en'


def _is_pdf_request(message: str) -> bool:
    """
    Check if user is requesting the detailed PDF report.
    Uses LLM for intelligent interpretation instead of keyword matching.
    
    Args:
        message: User's message
        
    Returns:
        bool: True if user wants PDF, False otherwise
    """
    message_lower = message.lower().strip()
    
    # If message is longer than 10 words, it's likely a new query, not a PDF request
    if len(message.split()) > 10:
        logger.info(f"ğŸ“ Message too long ({len(message.split())} words) - likely not a PDF request")
        return False
    
    # âœ… USE LLM FOR INTELLIGENT CLASSIFICATION (No hardcoded keywords!)
    try:
        from utils.call_llm import call_llm
        
        classification_prompt = f"""You are analyzing a user's response to a PDF offer in a WhatsApp conversation.

CONTEXT: The bot just offered to send a detailed PDF report and asked "Would you like a PDF report?"

USER'S RESPONSE: "{message}"

TASK: Classify this response as either:
- AFFIRMATIVE: User wants the PDF (says yes, agrees, requests it, etc.)
- NOT_AFFIRMATIVE: User is asking a new question, declining, or making any other statement (NOT requesting the PDF)

IMPORTANT RULES:
1. If user clearly agrees (like "yes", "haan", "sure", "send it", "please", "ji"), classify as AFFIRMATIVE
2. If user asks a NEW legal question (like "what about eviction?"), classify as NOT_AFFIRMATIVE
3. If user sends a greeting or irrelevant message (like "hi", "hello"), classify as NOT_AFFIRMATIVE
4. If user declines (like "no", "nahi", "later", "maybe later"), classify as NOT_AFFIRMATIVE
5. Consider cultural context: Urdu/English mixed responses
6. If unsure, classify as NOT_AFFIRMATIVE (safer to treat as new query)

EXAMPLES:
AFFIRMATIVE:
- "yes" â†’ AFFIRMATIVE
- "haan" â†’ AFFIRMATIVE
- "han" â†’ AFFIRMATIVE
- "ji" â†’ AFFIRMATIVE
- "sure" â†’ AFFIRMATIVE
- "ok" â†’ AFFIRMATIVE
- "send it" â†’ AFFIRMATIVE
- "please send pdf" â†’ AFFIRMATIVE
- "ÛØ§Úº" â†’ AFFIRMATIVE
- "Ø¬ÛŒ" â†’ AFFIRMATIVE
- "Ø¶Ø±ÙˆØ±" â†’ AFFIRMATIVE

NOT_AFFIRMATIVE:
- "what about property law?" â†’ NOT_AFFIRMATIVE
- "can i evict a tenant?" â†’ NOT_AFFIRMATIVE
- "no" â†’ NOT_AFFIRMATIVE
- "nahi" â†’ NOT_AFFIRMATIVE
- "Ù†ÛÛŒÚº" â†’ NOT_AFFIRMATIVE
- "later" â†’ NOT_AFFIRMATIVE
- "maybe later" â†’ NOT_AFFIRMATIVE
- "hi" â†’ NOT_AFFIRMATIVE
- "hello" â†’ NOT_AFFIRMATIVE

Respond with ONLY one word: "AFFIRMATIVE" or "NOT_AFFIRMATIVE"

CLASSIFICATION:"""

        result = call_llm(classification_prompt).strip().upper()
        
        # Parse result
        is_affirmative = "AFFIRMATIVE" in result and "NOT_AFFIRMATIVE" not in result
        
        if is_affirmative:
            logger.info(f"ğŸ¤– LLM classified as AFFIRMATIVE: '{message[:50]}'")
        else:
            logger.info(f"âœ… LLM classified as NOT_AFFIRMATIVE: '{message[:50]}'")
        
        return is_affirmative
        
    except Exception as e:
        logger.error(f"âŒ Error in LLM classification for affirmative: {e}")
        
        # âš ï¸ FALLBACK: Keyword matching (only if LLM fails!)
        logger.info("âš ï¸ Falling back to keyword-based affirmative detection")
        
        # Quick check for very obvious affirmatives
        obvious_yes = ['yes', 'yeah', 'yep', 'haan', 'han', 'ji', 'ÛØ§Úº', 'Ø¬ÛŒ']
        if message_lower in obvious_yes:
            logger.info(f"âš ï¸ Fallback quick match: '{message_lower}'")
            return True
        
        # English affirmatives
        english_yes = ['yes', 'yeah', 'yep', 'sure', 'ok', 'okay', 'send', 'please']
        
        # Urdu affirmatives (romanized and script)
        urdu_yes = ['haan', 'haa', 'han', 'ji', 'jee', 'zaroor', 'ÛØ§Úº', 'Ø¬ÛŒ', 'Ø¶Ø±ÙˆØ±']
        
        words = message_lower.split()
        
        # Check if any affirmative word appears
        for yes_word in english_yes + urdu_yes:
            if yes_word in words:
                logger.info(f"âš ï¸ Fallback keyword match: '{yes_word}'")
                return True
        
        # If message is very short (1-2 words) and not negative, assume yes
        if len(words) <= 2:
            negatives = ['no', 'nah', 'nope', 'nahi', 'Ù†ÛÛŒÚº']
            if not any(neg in words for neg in negatives):
                logger.info(f"âš ï¸ Fallback: Short message without negatives")
                return True
        
        return False


def _is_pdf_rejection(message: str) -> bool:
    """
    Check if user is rejecting/declining the PDF offer.
    Uses LLM for intelligent interpretation instead of keyword matching.
    
    Args:
        message: User's message
        
    Returns:
        bool: True if user doesn't want PDF, False otherwise
    """
    message_lower = message.lower().strip()
    
    # If message is longer than 10 words, it's likely a new query, not a rejection
    # This avoids unnecessary LLM calls for obvious legal queries
    if len(message.split()) > 10:
        logger.info(f"ğŸ“ Message too long ({len(message.split())} words) - likely not a rejection")
        return False
    
    # âœ… USE LLM FOR INTELLIGENT CLASSIFICATION (No hardcoded keywords!)
    try:
        from utils.call_llm import call_llm
        
        classification_prompt = f"""You are analyzing a user's response to a PDF offer in a WhatsApp conversation.

CONTEXT: The bot just offered to send a detailed PDF report and asked "Would you like a PDF report?"

USER'S RESPONSE: "{message}"

TASK: Classify this response as either:
- REJECTION: User clearly doesn't want the PDF (says no, declines, not interested, maybe later, skip, etc.)
- NOT_REJECTION: User is asking a new question, greeting, or making any other statement (NOT declining the PDF)

IMPORTANT RULES:
1. If user asks a NEW legal question (like "what about eviction?" or "can i evict a tenant?"), classify as NOT_REJECTION
2. If user sends a greeting (like "hi", "hello", "thanks", "thank you"), classify as NOT_REJECTION  
3. If user clearly declines (like "no", "nahi", "not now", "maybe later", "skip it"), classify as REJECTION
4. Consider cultural context: Urdu/English mixed responses
5. If unsure, classify as NOT_REJECTION (safer to process as new query than miss it)

EXAMPLES:
REJECTION:
- "no" â†’ REJECTION
- "nah" â†’ REJECTION
- "nope" â†’ REJECTION
- "nahi" â†’ REJECTION
- "Ù†ÛÛŒÚº" â†’ REJECTION
- "not now" â†’ REJECTION
- "maybe later" â†’ REJECTION
- "not interested" â†’ REJECTION
- "skip it" â†’ REJECTION
- "pass" â†’ REJECTION

NOT_REJECTION:
- "what about property law?" â†’ NOT_REJECTION
- "can i evict a tenant?" â†’ NOT_REJECTION
- "on what grounds can i evict?" â†’ NOT_REJECTION
- "hi" â†’ NOT_REJECTION
- "hello" â†’ NOT_REJECTION
- "thanks" â†’ NOT_REJECTION
- "thank you" â†’ NOT_REJECTION

Respond with ONLY one word: "REJECTION" or "NOT_REJECTION"

CLASSIFICATION:"""

        result = call_llm(classification_prompt).strip().upper()
        
        # Parse result
        is_rejection = "REJECTION" in result and "NOT_REJECTION" not in result
        
        if is_rejection:
            logger.info(f"ğŸ¤– LLM classified as REJECTION: '{message[:50]}'")
        else:
            logger.info(f"âœ… LLM classified as NOT_REJECTION: '{message[:50]}'")
        
        return is_rejection
        
    except Exception as e:
        logger.error(f"âŒ Error in LLM classification for rejection: {e}")
        
        # âš ï¸ FALLBACK: Word boundary matching (only if LLM fails!)
        logger.info("âš ï¸ Falling back to keyword-based rejection detection")
        
        # Quick check for very obvious rejections
        obvious_no = ['no', 'nah', 'nope', 'nahi', 'Ù†ÛÛŒÚº']
        if message_lower in obvious_no:
            logger.info(f"âš ï¸ Fallback quick match: '{message_lower}'")
            return True
        
        # English negatives
        english_no = ['no', 'nah', 'nope', 'not', 'dont', "don't", 'never', 'nvm', 
                      'skip', 'pass', 'later']
        
        # Urdu negatives (romanized and script)
        urdu_no = ['nahi', 'nhi', 'na', 'Ù†ÛÛŒÚº', 'Ù†Û']
        
        words = message_lower.split()
        
        # Check if any negative word appears as a COMPLETE WORD
        for neg_word in english_no + urdu_no:
            if neg_word in words:
                logger.info(f"âš ï¸ Fallback keyword match: '{neg_word}'")
                return True
        
        return False


def _handle_pdf_rejection(wa_id: str, detected_language: str) -> str:
    """
    Handle when user declines the PDF offer.
    
    Args:
        wa_id: WhatsApp user ID
        detected_language: Language of the conversation
        
    Returns:
        str: Friendly acknowledgment message
    """
    # âœ… CLEAR PENDING PDF STATE - Mark as declined
    try:
        chat_history = check_if_chat_exists(wa_id)
        if chat_history and len(chat_history) > 0:
            # Find and update the last message with pending PDF
            for msg in reversed(chat_history):
                if msg.get('role') == 'model' and 'research_data' in msg:
                    # Change state from pending to declined
                    msg['research_data']['type'] = 'pdf_declined'
                    break
            store_chat(wa_id, chat_history)
            logger.info("âœ… Marked PDF state as declined")
    except Exception as e:
        logger.error(f"Error updating PDF rejection status: {e}")
    
    # Return friendly message
    if detected_language == 'ur':
        return (
            "Ù¹Ú¾ÛŒÚ© ÛÛ’ØŒ Ú©ÙˆØ¦ÛŒ Ø¨Ø§Øª Ù†ÛÛŒÚº! ğŸ˜Š\n\n"
            "Ø§Ú¯Ø± Ø¢Ù¾ Ú©Ùˆ Ú©ÙˆØ¦ÛŒ Ø§ÙˆØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„ ÛÙˆ ØªÙˆ Ø¨Û’ Ø¬Ú¾Ø¬Ú¾Ú© Ù¾ÙˆÚ†Ú¾ÛŒÚºÛ” "
            "Ù…ÛŒÚº ÛŒÛØ§Úº Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Û’ Ù„ÛŒÛ’ ÛÙˆÚº! âš–ï¸"
        )
    elif detected_language == 'sd':
        return (
            "ÙºÙŠÚª Ø¢Ù‡ÙŠØŒ ÚªØ§ Ø¨Ù‡ Ú³Ø§Ù„Ù‡Ù‡ Ù†Ø§Ù‡ÙŠ! ğŸ˜Š\n\n"
            "Ø¬ÙŠÚªÚÙ‡Ù† ØªÙˆÙ‡Ø§Ù† Ú©ÙŠ ÚªÙˆ Ù»ÙŠÙˆ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³ÙˆØ§Ù„ Ø¢Ù‡ÙŠ ØªÙ‡ Ø¢Ø²Ø§Ø¯ÙŠ Ø³Ø§Ù† Ù¾Ú‡Ùˆ. "
            "Ø¢Ø¦ÙˆÙ† Ù‡ØªÙŠ ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ù…Ø¯Ø¯ Ù„Ø§Ø¡Ù Ø¢Ù‡ÙŠØ§Ù†! âš–ï¸"
        )
    elif detected_language == 'bl':
        return (
            "Ù¹Ú¾ÛŒÚ© ÛÛ’ØŒ Ú©ÙˆØ¦ÛŒ Ø¨Ø§Øª Ù†ÛÛŒÚº! ğŸ˜Š\n\n"
            "Ø§Ú¯Ø± Ø¢Ù¾ Ú©Ùˆ Ú©ÙˆØ¦ÛŒ Ø§ÙˆØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„ ÛÙˆ ØªÙˆ Ø¨Û’ Ø¬Ú¾Ø¬Ú¾Ú© Ù¾ÙˆÚ†Ú¾ÛŒÚºÛ” "
            "Ù…ÛŒÚº ÛŒÛØ§Úº Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Û’ Ù„ÛŒÛ’ ÛÙˆÚº! âš–ï¸"
        )
    else:
        return (
            "No problem at all! ğŸ˜Š\n\n"
            "If you have any other legal questions, feel free to ask. "
            "I'm here to help! âš–ï¸"
        )


def generate_pdf_report(wa_id: str, name: str, research_data: dict) -> str:
    """
    Generate a comprehensive, professionally formatted PDF report with enhanced styling and detailed legal analysis.

    Args:
        wa_id: WhatsApp user ID
        name: User's name
        research_data: Stored research data from previous query

    Returns:
        str: Path to generated PDF file
    """
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle
        from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER, TA_RIGHT, TA_LEFT
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.lib.colors import HexColor, black, white, gray, blue, darkblue, lightgrey
        import tempfile
        from datetime import datetime

        # Register Urdu-compatible font using available system fonts
        urdu_font = 'Helvetica'  # Default fallback

        try:
            # Priority order: Arial Unicode MS (best) â†’ Arial â†’ Tahoma â†’ Candara Arabic â†’ Garamond
            font_options = [
                ('ArialUnicodeMS', r"C:\Windows\Fonts\ARIALUNI.ttf", "Arial Unicode MS"),
                ('Arial', r"C:\Windows\Fonts\arial.ttf", "Arial Regular"),
                ('Tahoma', r"C:\Windows\Fonts\tahoma.ttf", "Tahoma Regular"),
                ('Candarab', r"C:\Windows\Fonts\Candarab.ttf", "Candara Bold (Arabic)"),
                ('GaramondBold', r"C:\Windows\Fonts\GARABD.TTF", "Garamond Bold"),
            ]

            for font_name, font_path, display_name in font_options:
                if os.path.exists(font_path):
                    try:
                        pdfmetrics.registerFont(TTFont(font_name, font_path))
                        urdu_font = font_name
                        logger.info(f"âœ… Registered {display_name} font for Urdu support")
                        break  # Use the first available font
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to register {display_name}: {e}")
                        continue

            if urdu_font == 'Helvetica':
                logger.warning("âš ï¸ No suitable system fonts found for Urdu, using Helvetica fallback")

        except Exception as font_error:
            logger.warning(f"âš ï¸ Font registration failed: {font_error}, using Helvetica fallback")
            urdu_font = 'Helvetica'

        # Extract research data with proper encoding handling
        query = research_data.get('query', 'Legal Query')
        full_legal_response = research_data.get('full_legal_response', '')
        pdf_links = research_data.get('pdf_links', [])
        doc_count = research_data.get('doc_count', 0)
        detected_language = research_data.get('detected_language', 'en')
        voice_summary = research_data.get('voice_summary', '')

        # Ensure all text data is properly encoded
        try:
            query = query.encode('utf-8', errors='replace').decode('utf-8')
            full_legal_response = full_legal_response.encode('utf-8', errors='replace').decode('utf-8')
            voice_summary = voice_summary.encode('utf-8', errors='replace').decode('utf-8')
        except Exception as encoding_error:
            logger.warning(f"Text encoding issue: {encoding_error}")
            # Continue with original text if encoding fails

        # Function to detect if text contains Urdu characters
        def is_urdu_text(text):
            if not text:
                return False
            urdu_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or '\u0750' <= char <= '\u077F')
            return urdu_chars / len(text) > 0.1  # More than 10% Urdu characters

        # Function to get appropriate style for text
        def get_text_style(text):
            if not text:
                return 'BodyText'
            urdu_ratio = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or '\u0750' <= char <= '\u077F') / len(text)
            if urdu_ratio > 0.5:  # More than 50% Urdu characters
                return 'UrduContent'  # Right-aligned for pure Urdu
            elif urdu_ratio > 0.1:  # Some Urdu characters
                return 'UrduText'    # Left-aligned for mixed content
            else:
                return 'BodyText'    # English text

        # Create PDF file
        temp_dir = tempfile.gettempdir()
        pdf_filename = f"LawYaar_Comprehensive_Report_{wa_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        pdf_path = os.path.join(temp_dir, pdf_filename)

        # Create PDF document with custom page templates
        doc = SimpleDocTemplate(pdf_path, pagesize=A4,
                              rightMargin=50, leftMargin=50,
                              topMargin=80, bottomMargin=50)

        # Container for PDF elements
        story = []

        # Enhanced Styles with Professional Formatting
        styles = getSampleStyleSheet()

        # Title Style
        styles.add(ParagraphStyle(
            name='ReportTitle',
            parent=styles['Title'],
            fontName='Helvetica-Bold',
            fontSize=24,
            alignment=TA_CENTER,
            textColor=darkblue,
            spaceAfter=20,
            leading=28
        ))

        # Subtitle Style
        styles.add(ParagraphStyle(
            name='ReportSubtitle',
            parent=styles['Heading2'],
            fontName='Helvetica-Bold',
            fontSize=14,
            alignment=TA_CENTER,
            textColor=blue,
            spaceAfter=15,
            leading=18
        ))

        # Section Header Style
        styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=styles['Heading1'],
            fontName='Helvetica-Bold',
            fontSize=16,
            alignment=TA_LEFT,
            textColor=darkblue,
            spaceBefore=15,
            spaceAfter=10,
            leading=20,
            borderColor=blue,
            borderWidth=0,
            borderPadding=5
        ))

        # Subsection Header Style
        styles.add(ParagraphStyle(
            name='SubsectionHeader',
            parent=styles['Heading2'],
            fontName='Helvetica-Bold',
            fontSize=14,
            alignment=TA_LEFT,
            textColor=HexColor('#2E4057'),
            spaceBefore=10,
            spaceAfter=8,
            leading=18
        ))

        # Body Text Style with better font handling
        styles.add(ParagraphStyle(
            name='BodyText',
            parent=styles['Normal'],
            fontName=urdu_font,
            fontSize=11,
            alignment=TA_JUSTIFY,
            leading=14,
            spaceAfter=6,
            encoding='utf-8'  # Ensure UTF-8 encoding
        ))

        # Urdu-specific text style for better word formation
        styles.add(ParagraphStyle(
            name='UrduText',
            parent=styles['Normal'],
            fontName=urdu_font,
            fontSize=13,  # Slightly larger for better readability
            alignment=TA_LEFT,  # Left-aligned for mixed content, but can be changed to TA_RIGHT for pure Urdu
            leading=16,
            spaceAfter=8,
            encoding='utf-8',
            wordSpace=2,  # Better word spacing for Urdu
            allowWidows=0,  # Prevent single lines at page breaks
            allowOrphans=0,  # Prevent orphaned lines
        ))

        # Urdu title style
        styles.add(ParagraphStyle(
            name='UrduTitle',
            parent=styles['Heading2'],
            fontName=urdu_font,
            fontSize=16,
            alignment=TA_LEFT,
            textColor=darkblue,
            spaceAfter=10,
            leading=20,
            encoding='utf-8'
        ))

        # Pure Urdu content style (right-aligned)
        styles.add(ParagraphStyle(
            name='UrduContent',
            parent=styles['Normal'],
            fontName=urdu_font,
            fontSize=14,
            alignment=TA_RIGHT,  # Right-aligned for pure Urdu content
            leading=18,
            spaceAfter=8,
            encoding='utf-8',
            wordSpace=3,  # More spacing for Urdu words
        ))

        # Metadata Style
        styles.add(ParagraphStyle(
            name='Metadata',
            parent=styles['Normal'],
            fontName='Helvetica',
            fontSize=10,
            alignment=TA_LEFT,
            textColor=HexColor('#34495E'),
            leading=12
        ))

        # Footer Style
        styles.add(ParagraphStyle(
            name='Footer',
            parent=styles['Normal'],
            fontName='Helvetica-Oblique',
            fontSize=9,
            alignment=TA_CENTER,
            textColor=gray,
            leading=11
        ))

        # Highlight Box Style
        styles.add(ParagraphStyle(
            name='HighlightBox',
            parent=styles['Normal'],
            fontName='Helvetica-Bold',
            fontSize=12,
            alignment=TA_LEFT,
            textColor=white,
            backColor=HexColor('#3498DB'),
            borderColor=HexColor('#2980B9'),
            borderWidth=1,
            borderPadding=8,
            leading=16
        ))

        # ================================
        # HEADER SECTION - Professional Branding (Localized)
        # ================================

        # Main Title with Enhanced Styling (Localized)
        if detected_language == 'ur':
            title_text = "ğŸ›ï¸ Ù„Ø§Ø¡ ÛŒØ§Ø± Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ø±Ù¾ÙˆØ±Ù¹"
            subtitle_text = "Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ú©ÛŒØ³ Ù„Ø§Ø¡ Ú©Ø§ Ø¬Ø§Ù…Ø¹ ØªØ¬Ø²ÛŒÛ"
        elif detected_language == 'sd':
            title_text = "ğŸ›ï¸ Ù„Ø§Ø¡ ÙŠØ§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¬ÙŠ Ø±Ù¾ÙˆØ±Ù½"
            subtitle_text = "Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù½ Ø¢Ù Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¬ÙŠ ÚªÙŠØ³ Ù„Ø§Ø¡ Ø¬Ùˆ Ø¬Ø§Ù…Ø¹ ØªØ¬Ø²ÙŠÙˆ"
        elif detected_language == 'bl':
            title_text = "ğŸ›ï¸ Ù„Ø§Ø¡ ÛŒØ§Ø± Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ø±Ù¾ÙˆØ±Ù¹"
            subtitle_text = "Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ú©ÛŒØ³ Ù„Ø§Ø¡ Ú©Ø§ Ø¬Ø§Ù…Ø¹ ØªØ¬Ø²ÛŒÛ"
        else:
            title_text = "ğŸ›ï¸ LAWYAAR LEGAL RESEARCH REPORT"
            subtitle_text = "Comprehensive Supreme Court of Pakistan Case Law Analysis"

        title = Paragraph(title_text, styles['ReportTitle'])
        story.append(title)

        subtitle = Paragraph(subtitle_text, styles['ReportSubtitle'])
        story.append(subtitle)

        # Decorative line
        story.append(Spacer(1, 10))

        # ================================
        # METADATA SECTION - Enhanced Presentation (Localized)
        # ================================

        # Create metadata table for better organization
        current_time = datetime.now()

        if detected_language == 'ur':
            metadata_data = [
                ['ğŸ“‹ Ø±Ù¾ÙˆØ±Ù¹ Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª', ''],
                ['Ø¬Ù†Ø§Ø¨ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ú©ÛŒ Ú¯Ø¦ÛŒ:', name],
                ['Ø±Ù¾ÙˆØ±Ù¹ Ú©ÛŒ ØªØ§Ø±ÛŒØ®:', current_time.strftime('%B %d, %Y')],
                ['Ø±Ù¾ÙˆØ±Ù¹ Ú©Ø§ ÙˆÙ‚Øª:', current_time.strftime('%I:%M %p')],
                ['Ú©ÛŒØ³Ø² Ú©Ø§ ØªØ¬Ø²ÛŒÛ:', f'{doc_count} Ù…ØªØ¹Ù„Ù‚Û Ú©ÛŒØ³Ø²'],
                ['Ø²Ø¨Ø§Ù†:', 'Ø§Ø±Ø¯Ùˆ/Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ'],
                ['Ø±Ù¾ÙˆØ±Ù¹ Ø¢Ø¦ÛŒ ÚˆÛŒ:', f'LY-{wa_id}-{current_time.strftime("%Y%m%d%H%M")}'],
            ]
        elif detected_language == 'sd':
            metadata_data = [
                ['ğŸ“‹ Ø±Ù¾ÙˆØ±Ù½ Ø¬ÙŠ ØªÙØµÙŠÙ„', ''],
                ['Ø¬Ù†Ø§Ø¨ Ù„Ø§Ø¡Ù ØªÙŠØ§Ø± ÚªÙŠÙ„:', name],
                ['Ø±Ù¾ÙˆØ±Ù½ Ø¬ÙŠ ØªØ§Ø±ÙŠØ®:', current_time.strftime('%B %d, %Y')],
                ['Ø±Ù¾ÙˆØ±Ù½ Ø¬Ùˆ ÙˆÙ‚Øª:', current_time.strftime('%I:%M %p')],
                ['ÚªÙŠØ³Ø² Ø¬Ùˆ ØªØ¬Ø²ÙŠÙˆ:', f'{doc_count} Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ ÚªÙŠØ³'],
                ['Ù»ÙˆÙ„ÙŠ:', 'Ø³Ù†ÚŒÙŠ/Ø§Ù†Ú¯Ø±ÙŠØ²ÙŠ'],
                ['Ø±Ù¾ÙˆØ±Ù½ Ø¢Ø¡Ù ÚŠÙŠ:', f'LY-{wa_id}-{current_time.strftime("%Y%m%d%H%M")}'],
            ]
        elif detected_language == 'bl':
            metadata_data = [
                ['ğŸ“‹ Ø±Ù¾ÙˆØ±Ù¹ Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª', ''],
                ['Ø¬Ù†Ø§Ø¨ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ú©ÛŒ Ú¯Ø¦ÛŒ:', name],
                ['Ø±Ù¾ÙˆØ±Ù¹ Ú©ÛŒ ØªØ§Ø±ÛŒØ®:', current_time.strftime('%B %d, %Y')],
                ['Ø±Ù¾ÙˆØ±Ù¹ Ú©Ø§ ÙˆÙ‚Øª:', current_time.strftime('%I:%M %p')],
                ['Ú©ÛŒØ³Ø² Ú©Ø§ ØªØ¬Ø²ÛŒÛ:', f'{doc_count} Ù…ØªØ¹Ù„Ù‚Û Ú©ÛŒØ³Ø²'],
                ['Ø²Ø¨Ø§Ù†:', 'Ø¨Ù„ÙˆÚ†ÛŒ/Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ'],
                ['Ø±Ù¾ÙˆØ±Ù¹ Ø¢Ø¦ÛŒ ÚˆÛŒ:', f'LY-{wa_id}-{current_time.strftime("%Y%m%d%H%M")}'],
            ]
        else:
            metadata_data = [
                ['ğŸ“‹ Report Details', ''],
                ['Generated for:', name],
                ['Report Date:', current_time.strftime('%B %d, %Y')],
                ['Report Time:', current_time.strftime('%I:%M %p')],
                ['Cases Analyzed:', f'{doc_count} relevant cases'],
                ['Language:', 'English'],
                ['Report ID:', f'LY-{wa_id}-{current_time.strftime("%Y%m%d%H%M")}'],
            ]

        # Create table with styling
        metadata_table = Table(metadata_data, colWidths=[2*inch, 4*inch])
        metadata_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), HexColor('#ECF0F1')),
            ('TEXTCOLOR', (0, 0), (-1, 0), darkblue),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 12),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
            ('FONTSIZE', (0, 1), (-1, -1), 10),
            ('GRID', (0, 0), (-1, -1), 0.5, lightgrey),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('LEFTPADDING', (0, 0), (-1, -1), 8),
            ('RIGHTPADDING', (0, 0), (-1, -1), 8),
            ('TOPPADDING', (0, 0), (-1, -1), 6),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        ]))

        story.append(metadata_table)
        story.append(Spacer(1, 20))

        # ================================
        # TABLE OF CONTENTS
        # ================================

        toc_title = Paragraph("<b>ğŸ“– TABLE OF CONTENTS</b>", styles['SectionHeader'])
        story.append(toc_title)
        story.append(Spacer(1, 10))

        toc_items = [
            "1. Executive Summary",
            "2. Your Legal Query",
            "3. Detailed Legal Analysis",
            "4. Key Findings & Principles",
            "5. Case References & Citations",
            "6. Additional Resources",
            "7. Methodology & Disclaimers"
        ]

        for item in toc_items:
            toc_item = Paragraph(item, styles['BodyText'])
            story.append(toc_item)
            story.append(Spacer(1, 3))

        story.append(Spacer(1, 15))

        # ================================
        # EXECUTIVE SUMMARY SECTION
        # ================================

        story.append(PageBreak())
        if detected_language == 'ur':
            exec_summary_title = Paragraph("<b>1. ğŸ“Š Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ø³Ù…Ø±ÛŒ</b>", styles['SectionHeader'])
        elif detected_language == 'sd':
            exec_summary_title = Paragraph("<b>1. ğŸ“Š Ø§ÙŠÚ¯Ø²ÙŠÚªÙŠÙˆÙ½Ùˆ Ø³Ù…ÙŠØ±ÙŠ</b>", styles['SectionHeader'])
        elif detected_language == 'bl':
            exec_summary_title = Paragraph("<b>1. ğŸ“Š Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ø³Ù…Ø±ÛŒ</b>", styles['SectionHeader'])
        else:
            exec_summary_title = Paragraph("<b>1. ğŸ“Š EXECUTIVE SUMMARY</b>", styles['SectionHeader'])
        story.append(exec_summary_title)
        story.append(Spacer(1, 10))

        # Summary statistics in a highlighted box (Localized)
        if detected_language == 'ur':
            summary_stats = f"""
            <b>ğŸ” ØªØ­Ù‚ÛŒÙ‚ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û:</b><br/>
            â€¢ {doc_count} Ù…ØªØ¹Ù„Ù‚Û Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©Û’ Ú©ÛŒØ³Ø² Ú©Ø§ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ú¯ÛŒØ§<br/>
            â€¢ Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ¬Ø²ÛŒÛ ÙØ±Ø§ÛÙ… Ú©ÛŒØ§ Ú¯ÛŒØ§<br/>
            â€¢ ØªÙ…Ø§Ù… Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø§ÙˆØ± Ú©ÛŒØ³ Ù„Ù†Ú©Ø³ Ø´Ø§Ù…Ù„ ÛÛŒÚº<br/>
            â€¢ Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ú©Ø§ Ø§Ø·Ù„Ø§Ù‚ Ú©ÛŒØ§ Ú¯ÛŒØ§
            """
        elif detected_language == 'sd':
            summary_stats = f"""
            <b>ğŸ” ØªØ­Ù‚ÙŠÙ‚ Ø¬Ùˆ Ø¬Ø§Ø¦Ø²Ùˆ:</b><br/>
            â€¢ {doc_count} Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ø³Ù¾Ø±ÙŠÙ… ÚªÙˆØ±Ù½ Ø¬ÙŠ ÚªÙŠØ³Ø² Ø¬Ùˆ ØªØ¬Ø²ÙŠÙˆ ÚªÙŠÙˆ ÙˆÙŠÙˆ<br/>
            â€¢ Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ¬Ø²ÙŠÙˆ Ù…Ù‡ÙŠØ§ ÚªÙŠÙˆ ÙˆÙŠÙˆ<br/>
            â€¢ Ø³Ú€ Ø­ÙˆØ§Ù„Ø§ Û½ ÚªÙŠØ³ Ù„Ù†Úª Ø´Ø§Ù…Ù„ Ø¢Ù‡Ù†<br/>
            â€¢ Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¬ÙŠ Ø·Ø±ÙŠÙ‚ÙŠ Ø¬Ùˆ Ø§Ø·Ù„Ø§Ù‚ ÚªÙŠÙˆ ÙˆÙŠÙˆ
            """
        elif detected_language == 'bl':
            summary_stats = f"""
            <b>ğŸ” ØªØ­Ù‚ÛŒÙ‚ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û:</b><br/>
            â€¢ {doc_count} Ù…ØªØ¹Ù„Ù‚Û Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©Û’ Ú©ÛŒØ³Ø² Ú©Ø§ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ú¯ÛŒØ§<br/>
            â€¢ Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ¬Ø²ÛŒÛ ÙØ±Ø§ÛÙ… Ú©ÛŒØ§ Ú¯ÛŒØ§<br/>
            â€¢ ØªÙ…Ø§Ù… Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø§ÙˆØ± Ú©ÛŒØ³ Ù„Ù†Ú©Ø³ Ø´Ø§Ù…Ù„ ÛÛŒÚº<br/>
            â€¢ Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ú©Ø§ Ø§Ø·Ù„Ø§Ù‚ Ú©ÛŒØ§ Ú¯ÛŒØ§
            """
        else:
            summary_stats = f"""
            <b>ğŸ” Research Overview:</b><br/>
            â€¢ Analyzed {doc_count} relevant Supreme Court cases<br/>
            â€¢ Comprehensive legal analysis provided<br/>
            â€¢ All citations and case links included<br/>
            â€¢ Professional legal research methodology applied
            """

        summary_box = Paragraph(summary_stats, styles['HighlightBox'])
        story.append(summary_box)
        story.append(Spacer(1, 15))

        # Voice summary content
        if voice_summary.strip():
            if detected_language == 'ur':
                summary_content = Paragraph("<b>ğŸ’¡ Ú©Ù„ÛŒØ¯ÛŒ Ù†ØªØ§Ø¦Ø¬:</b>", styles['SubsectionHeader'])
            elif detected_language == 'sd':
                summary_content = Paragraph("<b>ğŸ’¡ ÚªÙ„ÙŠØ¯ÙŠ Ù†ØªÙŠØ¬Ø§:</b>", styles['SubsectionHeader'])
            elif detected_language == 'bl':
                summary_content = Paragraph("<b>ğŸ’¡ Ú©Ù„ÛŒØ¯ÛŒ Ù†ØªØ§Ø¦Ø¬:</b>", styles['SubsectionHeader'])
            else:
                summary_content = Paragraph("<b>ğŸ’¡ Key Findings:</b>", styles['SubsectionHeader'])
            story.append(summary_content)
            story.append(Spacer(1, 5))

            # Escape XML special characters in summary
            from xml.sax.saxutils import escape
            summary_escaped = escape(voice_summary)
            # Simple markdown conversion (bold only)
            import re
            summary_escaped = re.sub(r'\*\*([^*]+)\*\*', r'<b>\1</b>', summary_escaped)
            summary_escaped = re.sub(r'\*([^*]+)\*', r'<i>\1</i>', summary_escaped)

            style_name = get_text_style(voice_summary)
            story.append(Paragraph(summary_escaped, styles[style_name]))
            story.append(Spacer(1, 10))

        # ================================
        # USER QUERY SECTION (Localized)
        # ================================

        if detected_language == 'ur':
            query_title = Paragraph("<b>2. â“ Ø¢Ù¾ Ú©Ø§ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„</b>", styles['SectionHeader'])
        elif detected_language == 'sd':
            query_title = Paragraph("<b>2. â“ ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³ÙˆØ§Ù„</b>", styles['SectionHeader'])
        elif detected_language == 'bl':
            query_title = Paragraph("<b>2. â“ Ø¢Ù¾ Ú©Ø§ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„</b>", styles['SectionHeader'])
        else:
            query_title = Paragraph("<b>2. â“ YOUR LEGAL QUERY</b>", styles['SectionHeader'])
        story.append(query_title)
        story.append(Spacer(1, 10))

        # Query in a bordered box (Localized)
        if detected_language == 'ur':
            query_label = "Ø³ÙˆØ§Ù„:"
        elif detected_language == 'sd':
            query_label = "Ø³ÙˆØ§Ù„:"
        elif detected_language == 'bl':
            query_label = "Ø³ÙˆØ§Ù„:"
        else:
            query_label = "Query:"

        query_box_data = [[Paragraph(f"<b>{query_label}</b> {escape(query)}", styles['BodyText'])]]
        query_table = Table(query_box_data, colWidths=[7*inch])
        query_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, -1), HexColor('#F8F9FA')),
            ('BOX', (0, 0), (-1, -1), 1, HexColor('#DEE2E6')),
            ('LEFTPADDING', (0, 0), (-1, -1), 12),
            ('RIGHTPADDING', (0, 0), (-1, -1), 12),
            ('TOPPADDING', (0, 0), (-1, -1), 10),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 10),
        ]))

        story.append(query_table)
        story.append(Spacer(1, 15))

        # ================================
        # DETAILED LEGAL ANALYSIS (Localized)
        # ================================

        story.append(PageBreak())
        if detected_language == 'ur':
            analysis_title = Paragraph("<b>3. âš–ï¸ ØªÙØµÛŒÙ„ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ¬Ø²ÛŒÛ</b>", styles['SectionHeader'])
        elif detected_language == 'sd':
            analysis_title = Paragraph("<b>3. âš–ï¸ ØªÙØµÙŠÙ„ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ¬Ø²ÙŠÙˆ</b>", styles['SectionHeader'])
        elif detected_language == 'bl':
            analysis_title = Paragraph("<b>3. âš–ï¸ ØªÙØµÛŒÙ„ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ¬Ø²ÛŒÛ</b>", styles['SectionHeader'])
        else:
            analysis_title = Paragraph("<b>3. âš–ï¸ DETAILED LEGAL ANALYSIS</b>", styles['SectionHeader'])
        story.append(analysis_title)
        story.append(Spacer(1, 12))

        # Analysis introduction (Localized)
        if detected_language == 'ur':
            intro_text = """
            <b>Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ø§ÙˆØ± ØªØ¬Ø²ÛŒÛ</b><br/><br/>
            ÛŒÛ Ø³ÛŒÚ©Ø´Ù† Ø¢Ù¾ Ú©Û’ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„ Ú©Ø§ Ù…ØªØ¹Ù„Ù‚Û Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ú©ÛŒØ³ Ù„Ø§Ø¡ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ú¯ÛØ±Ø§Ø¦ÛŒ Ø³Û’ ØªØ¬Ø²ÛŒÛ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û”
            ØªØ¬Ø²ÛŒÛ Ù…ØªØ¹Ø¯Ø¯ Ú©ÛŒØ³Ø² Ø³Û’ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ø§ Ø§Ù…ØªØ²Ø§Ø¬ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ú©Ù„ÛŒØ¯ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø§ØµÙˆÙ„ÙˆÚºØŒ ÙÛŒØµÙ„ÙˆÚº Ø§ÙˆØ± Ø¹Ù…Ù„ÛŒ Ù…Ø¶Ù…Ø±Ø§Øª Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ØªØ§ ÛÛ’Û”
            """
        elif detected_language == 'sd':
            intro_text = """
            <b>Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Û½ ØªØ¬Ø²ÙŠÙˆ</b><br/><br/>
            Ù‡ÙŠ Ø³ÙŠÚªØ´Ù† ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³ÙˆØ§Ù„ Ø¬ÙŠ Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ø³Ù¾Ø±ÙŠÙ… ÚªÙˆØ±Ù½ Ø¢Ù Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¬ÙŠ ÚªÙŠØ³ Ù„Ø§Ø¡ Ø¬ÙŠ Ø¨Ù†ÙŠØ§Ø¯ ØªÙŠ Ú¯Ù‡Ø±Ø§Ø¦ÙŠØ¡Ù Ø³Ø§Ù† ØªØ¬Ø²ÙŠÙˆ Ù…Ù‡ÙŠØ§ ÚªØ±ÙŠ Ù¿Ùˆ.
            ØªØ¬Ø²ÙŠÙˆ Ú¯Ù‡Ú»Ù† ÚªÙŠØ³Ø² Ù…Ø§Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ùˆ Ù…ÙŠÙ„Ø§Ù¾ ÚªØ±ÙŠ Ù¿ÙˆØŒ ÚªÙ„ÙŠØ¯ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§ØµÙˆÙ„Ù†ØŒ ÙÙŠØµÙ„Ù† Û½ Ø¹Ù…Ù„ÙŠ Ù†ØªÙŠØ¬Ù† Ú©ÙŠ Ø§Ø¬Ø§Ú¯Ø± ÚªØ±ÙŠ Ù¿Ùˆ.
            """
        elif detected_language == 'bl':
            intro_text = """
            <b>Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ø§ÙˆØ± ØªØ¬Ø²ÛŒÛ</b><br/><br/>
            ÛŒÛ Ø³ÛŒÚ©Ø´Ù† Ø¢Ù¾ Ú©Û’ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÙˆØ§Ù„ Ú©Ø§ Ù…ØªØ¹Ù„Ù‚Û Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ú©ÛŒØ³ Ù„Ø§Ø¡ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ú¯ÛØ±Ø§Ø¦ÛŒ Ø³Û’ ØªØ¬Ø²ÛŒÛ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û”
            ØªØ¬Ø²ÛŒÛ Ù…ØªØ¹Ø¯Ø¯ Ú©ÛŒØ³Ø² Ø³Û’ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ø§ Ø§Ù…ØªØ²Ø§Ø¬ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ú©Ù„ÛŒØ¯ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø§ØµÙˆÙ„ÙˆÚºØŒ ÙÛŒØµÙ„ÙˆÚº Ø§ÙˆØ± Ø¹Ù…Ù„ÛŒ Ù…Ø¶Ù…Ø±Ø§Øª Ú©Ùˆ Ø§Ø¬Ø§Ú¯Ø± Ú©Ø±ØªØ§ ÛÛ’Û”
            """
        else:
            intro_text = """
            <b>Comprehensive Legal Research & Analysis</b><br/><br/>
            This section provides an in-depth analysis of your legal query based on relevant Supreme Court of Pakistan case law.
            The analysis synthesizes information from multiple cases, highlighting key legal principles, holdings, and practical implications.
            """
        story.append(Paragraph(intro_text, styles['BodyText']))
        story.append(Spacer(1, 10))

        # Main legal analysis content
        if full_legal_response.strip():
            try:
                # Escape and convert markdown to PDF-friendly format
                legal_text = escape(full_legal_response)

                # Enhanced markdown conversion
                legal_text = re.sub(r'\*\*([^*]+)\*\*', r'<b>\1</b>', legal_text)  # Bold
                legal_text = re.sub(r'\*([^*]+)\*', r'<i>\1</i>', legal_text)     # Italic
                legal_text = re.sub(r'^### (.*)$', r'<b>\1</b>', legal_text, flags=re.MULTILINE)  # H3 headers
                legal_text = re.sub(r'^## (.*)$', r'<b>\1</b>', legal_text, flags=re.MULTILINE)   # H2 headers
                legal_text = re.sub(r'^# (.*)$', r'<b>\1</b>', legal_text, flags=re.MULTILINE)    # H1 headers

                # Handle line breaks and paragraphs
                legal_text = legal_text.replace('\n\n', '<br/><br/>')
                legal_text = legal_text.replace('\n', '<br/>')

                # Ensure text is properly encoded for PDF
                legal_text = legal_text.encode('utf-8', errors='replace').decode('utf-8')

                # Split into paragraphs and add formatting
                paragraphs = legal_text.split('<br/><br/>')
                for i, para in enumerate(paragraphs):
                    para = para.strip()
                    if para and len(para) > 10:  # Filter out very short fragments
                        # Add paragraph numbering for main sections
                        if len(para) > 100:  # Substantial paragraphs
                            style_name = get_text_style(para)
                            story.append(Paragraph(f"<b>[{i+1}]</b> {para}", styles[style_name]))
                        else:
                            style_name = get_text_style(para)
                            story.append(Paragraph(para, styles[style_name]))
                        story.append(Spacer(1, 6))
            except Exception as text_error:
                logger.warning(f"Error processing legal text: {text_error}")
                # Fallback to simple text processing
                simple_text = escape(full_legal_response)
                style_name = get_text_style(simple_text)
                story.append(Paragraph(simple_text, styles[style_name]))
                story.append(Spacer(1, 6))

        # ================================
        # KEY FINDINGS SECTION
        # ================================

        story.append(PageBreak())
        findings_title = Paragraph("<b>4. ğŸ¯ KEY FINDINGS & LEGAL PRINCIPLES</b>", styles['SectionHeader'])
        story.append(findings_title)
        story.append(Spacer(1, 10))

        # Extract key points from the analysis (simplified version)
        findings_text = """
        <b>ğŸ“‹ Summary of Key Legal Principles:</b><br/><br/>
        â€¢ <b>Precedent Analysis:</b> Relevant case law has been examined and applied to your specific query<br/>
        â€¢ <b>Legal Reasoning:</b> Analysis follows established judicial reasoning and legal methodology<br/>
        â€¢ <b>Practical Application:</b> Findings are directly applicable to real-world legal scenarios<br/>
        â€¢ <b>Case Citations:</b> All conclusions are supported by specific Supreme Court precedents
        """

        story.append(Paragraph(findings_text, styles['BodyText']))
        story.append(Spacer(1, 15))

        # ================================
        # CASE REFERENCES SECTION (Localized)
        # ================================

        story.append(PageBreak())
        if pdf_links and len(pdf_links) > 0:
            if detected_language == 'ur':
                references_title = Paragraph("<b>5. ğŸ“š Ú©ÛŒØ³ Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø§ÙˆØ± Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª</b>", styles['SectionHeader'])
            elif detected_language == 'sd':
                references_title = Paragraph("<b>5. ğŸ“š ÚªÙŠØ³ Ø­ÙˆØ§Ù„Ø§ Û½ Ø§Ù‚ØªØ¨Ø§Ø³</b>", styles['SectionHeader'])
            elif detected_language == 'bl':
                references_title = Paragraph("<b>5. ğŸ“š Ú©ÛŒØ³ Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø§ÙˆØ± Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª</b>", styles['SectionHeader'])
            else:
                references_title = Paragraph("<b>5. ğŸ“š CASE REFERENCES & CITATIONS</b>", styles['SectionHeader'])
            story.append(references_title)
            story.append(Spacer(1, 12))

            if detected_language == 'ur':
                references_intro = f"""
                <b>Ú©ÛŒØ³ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Ø§ Ù…Ú©Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Û ({len(pdf_links)} Ú©ÛŒØ³Ø²)</b><br/><br/>
                Ø°ÛŒÙ„ Ù…ÛŒÚº Ø§Ø³ Ø±Ù¾ÙˆØ±Ù¹ Ù…ÛŒÚº ØªØ¬Ø²ÛŒÛ Ú©ÛŒÛ’ Ú¯Ø¦Û’ ØªÙ…Ø§Ù… Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©Û’ Ú©ÛŒØ³Ø² Ú©ÛŒ Ø¬Ø§Ù…Ø¹ ÙÛØ±Ø³Øª ÛÛ’Û”
                ÛØ± Ú©ÛŒØ³ Ù…ÛŒÚº Ø§Ù‚ØªØ¨Ø§Ø³ Ú©ÛŒ ØªÙØµÛŒÙ„Ø§ØªØŒ Ú©ÛŒØ³ Ú©Û’ Ø¹Ù†ÙˆØ§Ù†Ø§Øª Ø§ÙˆØ± Ø³Ø±Ú©Ø§Ø±ÛŒ Ø¹Ø¯Ø§Ù„Øª Ú©Û’ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Û’ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ù„Ù†Ú©Ø³ Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”
                """
            elif detected_language == 'sd':
                references_intro = f"""
                <b>ÚªÙŠØ³ Ø¯Ø³ØªØ§ÙˆÙŠØ² Ø¬Ùˆ Ù…ÚªÙ…Ù„ Ù…ÙŠÙ„Ø§Ù¾ ({len(pdf_links)} ÚªÙŠØ³)</b><br/><br/>
                Ù‡ÙŠÙº Ù‡Ù† Ø±Ù¾ÙˆØ±Ù½ Û¾ ØªØ¬Ø²ÙŠÙˆ ÚªÙŠÙ„ Ø³Ú€Ù†ÙŠ Ø³Ù¾Ø±ÙŠÙ… ÚªÙˆØ±Ù½ Ø¬ÙŠ ÚªÙŠØ³Ø² Ø¬ÙŠ Ø¬Ø§Ù…Ø¹ ÙÙ‡Ø±Ø³Øª Ø¢Ù‡ÙŠ.
                Ù‡Ø± ÚªÙŠØ³ Û¾ Ø§Ù‚ØªØ¨Ø§Ø³ Ø¬ÙŠ ØªÙØµÙŠÙ„ØŒ ÚªÙŠØ³ Ø¬Ø§ Ø¹Ù†ÙˆØ§Ù† Û½ Ø³Ø±ÚªØ§Ø±ÙŠ Ø¹Ø¯Ø§Ù„Øª Ø¬ÙŠ Ø¯Ø³ØªØ§ÙˆÙŠØ² Ø¬ÙŠ Ø³ÚŒÙŠ Ù„Ù†Úª Ø´Ø§Ù…Ù„ Ø¢Ù‡Ù†.
                """
            elif detected_language == 'bl':
                references_intro = f"""
                <b>Ú©ÛŒØ³ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Ø§ Ù…Ú©Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Û ({len(pdf_links)} Ú©ÛŒØ³Ø²)</b><br/><br/>
                Ø°ÛŒÙ„ Ù…ÛŒÚº Ø§Ø³ Ø±Ù¾ÙˆØ±Ù¹ Ù…ÛŒÚº ØªØ¬Ø²ÛŒÛ Ú©ÛŒÛ’ Ú¯Ø¦Û’ ØªÙ…Ø§Ù… Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©Û’ Ú©ÛŒØ³Ø² Ú©ÛŒ Ø¬Ø§Ù…Ø¹ ÙÛØ±Ø³Øª ÛÛ’Û”
                ÛØ± Ú©ÛŒØ³ Ù…ÛŒÚº Ø§Ù‚ØªØ¨Ø§Ø³ Ú©ÛŒ ØªÙØµÛŒÙ„Ø§ØªØŒ Ú©ÛŒØ³ Ú©Û’ Ø¹Ù†ÙˆØ§Ù†Ø§Øª Ø§ÙˆØ± Ø³Ø±Ú©Ø§Ø±ÛŒ Ø¹Ø¯Ø§Ù„Øª Ú©Û’ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Û’ Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ù„Ù†Ú©Ø³ Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”
                """
            else:
                references_intro = f"""
                <b>Complete Case Documentation ({len(pdf_links)} cases)</b><br/><br/>
                Below is a comprehensive list of all Supreme Court cases analyzed in this report.
                Each case includes citation details, case titles, and direct links to official court documents.
                """
            story.append(Paragraph(references_intro, styles['BodyText']))
            story.append(Spacer(1, 10))

            # Create case reference cards (Localized labels)
            for i, pdf_info in enumerate(pdf_links, 1):
                case_no = pdf_info.get('case_no', 'Case')
                case_title = pdf_info.get('title', '')
                url = pdf_info.get('url', '')

                # Case card in a table format (Localized)
                if detected_language == 'ur':
                    case_data = [
                        [f'ğŸ›ï¸ Ú©ÛŒØ³ {i}: {case_no}', ''],
                        ['Ø¹Ù†ÙˆØ§Ù†:', case_title if case_title else 'N/A'],
                        ['Ø§Ù‚ØªØ¨Ø§Ø³:', case_no],
                        ['Ø¯Ø³ØªØ§ÙˆÛŒØ² Ù„Ù†Ú©:', f'<a href="{url}">{url}</a>' if url else 'N/A'],
                    ]
                elif detected_language == 'sd':
                    case_data = [
                        [f'ğŸ›ï¸ ÚªÙŠØ³ {i}: {case_no}', ''],
                        ['Ø¹Ù†ÙˆØ§Ù†:', case_title if case_title else 'N/A'],
                        ['Ø§Ù‚ØªØ¨Ø§Ø³:', case_no],
                        ['Ø¯Ø³ØªØ§ÙˆÙŠØ² Ù„Ù†Úª:', f'<a href="{url}">{url}</a>' if url else 'N/A'],
                    ]
                elif detected_language == 'bl':
                    case_data = [
                        [f'ğŸ›ï¸ Ú©ÛŒØ³ {i}: {case_no}', ''],
                        ['Ø¹Ù†ÙˆØ§Ù†:', case_title if case_title else 'N/A'],
                        ['Ø§Ù‚ØªØ¨Ø§Ø³:', case_no],
                        ['Ø¯Ø³ØªØ§ÙˆÛŒØ² Ù„Ù†Ú©:', f'<a href="{url}">{url}</a>' if url else 'N/A'],
                    ]
                else:
                    case_data = [
                        [f'ğŸ›ï¸ Case {i}: {case_no}', ''],
                        ['Title:', case_title if case_title else 'N/A'],
                        ['Citation:', case_no],
                        ['Document Link:', f'<a href="{url}">{url}</a>' if url else 'N/A'],
                    ]

                case_table = Table(case_data, colWidths=[1.5*inch, 5.5*inch])
                case_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), HexColor('#3498DB')),
                    ('TEXTCOLOR', (0, 0), (-1, 0), white),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), HexColor('#ECF0F1')),
                    ('TEXTCOLOR', (0, 1), (-1, -1), black),
                    ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),
                    ('FONTSIZE', (0, 1), (-1, -1), 10),
                    ('GRID', (0, 0), (-1, -1), 0.5, lightgrey),
                    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
                    ('LEFTPADDING', (0, 0), (-1, -1), 8),
                    ('RIGHTPADDING', (0, 0), (-1, -1), 8),
                    ('TOPPADDING', (0, 0), (-1, -1), 6),
                    ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
                ]))

                story.append(case_table)
                story.append(Spacer(1, 12))

        # ================================
        # ADDITIONAL RESOURCES SECTION (Localized)
        # ================================

        story.append(PageBreak())
        if detected_language == 'ur':
            resources_title = Paragraph("<b>6. ğŸ“– Ø§Ø¶Ø§ÙÛŒ ÙˆØ³Ø§Ø¦Ù„</b>", styles['SectionHeader'])
        elif detected_language == 'sd':
            resources_title = Paragraph("<b>6. ğŸ“– Ø§Ø¶Ø§ÙÙŠ ÙˆØ³ÙŠÙ„Ø§</b>", styles['SectionHeader'])
        elif detected_language == 'bl':
            resources_title = Paragraph("<b>6. ğŸ“– Ø§Ø¶Ø§ÙÛŒ ÙˆØ³Ø§Ø¦Ù„</b>", styles['SectionHeader'])
        else:
            resources_title = Paragraph("<b>6. ğŸ“– ADDITIONAL RESOURCES</b>", styles['SectionHeader'])
        story.append(resources_title)
        story.append(Spacer(1, 12))

        if detected_language == 'ur':
            resources_text = """
            <b>ğŸ”— Ù…ÙÛŒØ¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÙˆØ³Ø§Ø¦Ù„:</b><br/><br/>
            â€¢ <b>Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ø³Ø±Ú©Ø§Ø±ÛŒ ÙˆÛŒØ¨ Ø³Ø§Ø¦Ù¹:</b> www.supremecourt.gov.pk<br/>
            â€¢ <b>Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ø§ Ù…Ø±Ú©Ø²:</b> www.paklii.org<br/>
            â€¢ <b>Ù‚Ø§Ù†ÙˆÙ† Ø§ÙˆØ± Ø§Ù†ØµØ§Ù Ú©Ù…ÛŒØ´Ù† Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù†:</b> www.ljcp.gov.pk<br/>
            â€¢ <b>Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Û’ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³:</b> Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨Ø§Ø± Ú©ÙˆÙ†Ø³Ù„ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¯Ø³ØªÛŒØ§Ø¨<br/><br/>
            <b>ğŸ’¼ Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø®Ø¯Ù…Ø§Øª:</b><br/>
            Ø°Ø§ØªÛŒ Ù†ÙˆØ¹ÛŒØª Ú©ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø´Ø§ÙˆØ±Øª Ø§ÙˆØ± Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø§ÛÙ„ ÙˆÚ©ÛŒÙ„ÛŒÙ† Ø§ÙˆØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù¾ÛŒØ´Û ÙˆØ±Ø¤Úº Ø³Û’ Ø±Ø¬ÙˆØ¹ Ú©Ø±ÛŒÚºÛ”
            """
        elif detected_language == 'sd':
            resources_text = """
            <b>ğŸ”— Ù…ÙÙŠØ¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ³ÙŠÙ„Ø§:</b><br/><br/>
            â€¢ <b>Ø³Ù¾Ø±ÙŠÙ… ÚªÙˆØ±Ù½ Ø¢Ù Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¬ÙŠ Ø³Ø±ÚªØ§Ø±ÙŠ ÙˆÙŠØ¨ Ø³Ø§Ø¦ÙŠÙ½:</b> www.supremecourt.gov.pk<br/>
            â€¢ <b>Ù¾Ø§ÚªØ³ØªØ§Ù† Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ùˆ Ù…Ø±ÚªØ²:</b> www.paklii.org<br/>
            â€¢ <b>Ù‚Ø§Ù†ÙˆÙ† Û½ Ø§Ù†ØµØ§Ù ÚªÙ…ÙŠØ´Ù† Ø¢Ù Ù¾Ø§ÚªØ³ØªØ§Ù†:</b> www.ljcp.gov.pk<br/>
            â€¢ <b>Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¬Ø§ ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³:</b> Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¨Ø§Ø± ÚªÙˆÙ†Ø³Ù„ Ø¬ÙŠ Ø°Ø±ÙŠØ¹ÙŠ Ø¯Ø³ØªÙŠØ§Ø¨<br/><br/>
            <b>ğŸ’¼ Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø®Ø¯Ù…ØªÙˆÙ†:</b><br/>
            Ø°Ø§ØªÙŠ Ù†ÙˆØ¹ÙŠØª Ø¬ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØµÙ„Ø§Ø­ Û½ Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÙŠ Ù„Ø§Ø¡ÙØŒ Ø§Ù‡Ù„ ÙˆÚªÙŠÙ„Ù† Û½ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„Ø² Ø³Ø§Ù† Ø±Ø¬ÙˆØ¹ ÚªØ±ÙŠÙˆ.
            """
        elif detected_language == 'bl':
            resources_text = """
            <b>ğŸ”— Ù…ÙÛŒØ¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÙˆØ³Ø§Ø¦Ù„:</b><br/><br/>
            â€¢ <b>Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ø³Ø±Ú©Ø§Ø±ÛŒ ÙˆÛŒØ¨ Ø³Ø§Ø¦Ù¹:</b> www.supremecourt.gov.pk<br/>
            â€¢ <b>Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ø§ Ù…Ø±Ú©Ø²:</b> www.paklii.org<br/>
            â€¢ <b>Ù‚Ø§Ù†ÙˆÙ† Ø§ÙˆØ± Ø§Ù†ØµØ§Ù Ú©Ù…ÛŒØ´Ù† Ø¢Ù Ù¾Ø§Ú©Ø³ØªØ§Ù†:</b> www.ljcp.gov.pk<br/>
            â€¢ <b>Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Û’ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³:</b> Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨Ø§Ø± Ú©ÙˆÙ†Ø³Ù„ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¯Ø³ØªÛŒØ§Ø¨<br/><br/>
            <b>ğŸ’¼ Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø®Ø¯Ù…Ø§Øª:</b><br/>
            Ø°Ø§ØªÛŒ Ù†ÙˆØ¹ÛŒØª Ú©ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø´Ø§ÙˆØ±Øª Ø§ÙˆØ± Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø§ÛÙ„ ÙˆÚ©ÛŒÙ„ÛŒÙ† Ø§ÙˆØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù¾ÛŒØ´Û ÙˆØ±Ø¤Úº Ø³Û’ Ø±Ø¬ÙˆØ¹ Ú©Ø±ÛŒÚºÛ”
            """
        else:
            resources_text = """
            <b>ğŸ”— Useful Legal Resources:</b><br/><br/>
            â€¢ <b>Supreme Court of Pakistan Official Website:</b> www.supremecourt.gov.pk<br/>
            â€¢ <b>Pakistan Legal Information Center:</b> www.paklii.org<br/>
            â€¢ <b>Law & Justice Commission of Pakistan:</b> www.ljcp.gov.pk<br/>
            â€¢ <b>Legal Research Databases:</b> Available through Pakistan Bar Council<br/><br/>
            <b>ğŸ’¼ Professional Legal Services:</b><br/>
            For personalized legal advice and representation, consult qualified attorneys and legal professionals.
            """

        story.append(Paragraph(resources_text, styles['BodyText']))
        story.append(Spacer(1, 15))

        # ================================
        # METHODOLOGY & DISCLAIMERS (Localized)
        # ================================

        story.append(PageBreak())
        if detected_language == 'ur':
            methodology_title = Paragraph("<b>7. ğŸ”¬ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ø§ÙˆØ± Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ø§Ø®Ø·Ø§Ø±ÛŒÛ’</b>", styles['SectionHeader'])
        elif detected_language == 'sd':
            methodology_title = Paragraph("<b>7. ğŸ”¬ Ø·Ø±ÙŠÙ‚Ùˆ ÚªØ§Ø± Û½ Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„ Ø§Ø®ØªÙŠØ§Ø±ÙŠÙˆÙ†</b>", styles['SectionHeader'])
        elif detected_language == 'bl':
            methodology_title = Paragraph("<b>7. ğŸ”¬ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ø§ÙˆØ± Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ø§Ø®Ø·Ø§Ø±ÛŒÛ’</b>", styles['SectionHeader'])
        else:
            methodology_title = Paragraph("<b>7. ğŸ”¬ METHODOLOGY & PROFESSIONAL DISCLAIMERS</b>", styles['SectionHeader'])
        story.append(methodology_title)
        story.append(Spacer(1, 12))

        if detected_language == 'ur':
            methodology_text = """
            <b>ğŸ¤– AI Ú©ÛŒ Ø·Ø§Ù‚Øª Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø±:</b><br/><br/>
            ÛŒÛ Ø±Ù¾ÙˆØ±Ù¹ Ø¬Ø¯ÛŒØ¯ AI Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ ØªÛŒØ§Ø± Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’:<br/>
            â€¢ <b>ÙˆÛŒÚ©Ù¹Ø± ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³ Ú©ÛŒ ØªÙ„Ø§Ø´:</b> Ø§ÛŒÙ…Ø¨ÛŒÚˆÙ†Ú¯Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©ÛŒØ³ Ù„Ø§Ø¡ Ú©Ø§ Ø³ÛŒÙ…Ù†Ù¹Ú© ØªØ¬Ø²ÛŒÛ<br/>
            â€¢ <b>LLM ØªØ¬Ø²ÛŒÛ:</b> Ø¨Ú‘Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø§ØµÙˆÙ„ÙˆÚº Ú©Ø§ ØªØ¬Ø²ÛŒÛ<br/>
            â€¢ <b>Ù…Ø§Ø®Ø° Ú©ÛŒ ØªØµØ¯ÛŒÙ‚:</b> ØªÙ…Ø§Ù… Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø³Ø±Ú©Ø§Ø±ÛŒ Ø¹Ø¯Ø§Ù„Øª Ú©Û’ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ø³Û’ Ù…Ù†Ø³Ù„Ú© ÛÛŒÚº<br/>
            â€¢ <b>Ú©ÙˆØ§Ù„Ù¹ÛŒ Ú©ÛŒ ÛŒÙ‚ÛŒÙ† Ø¯ÛØ§Ù†ÛŒ:</b> Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©ÛŒ Ú©Ø«ÛŒØ± Ù…Ø±Ø­Ù„Û ØªØµØ¯ÛŒÙ‚<br/><br/>
            <b>âš ï¸ Ø§ÛÙ… Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ø§Ø®Ø·Ø§Ø±ÛŒÛ’:</b><br/><br/>
            â€¢ <i>ÛŒÛ Ø±Ù¾ÙˆØ±Ù¹ ØµØ±Ù Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÛŒ Ù…Ù‚Ø§ØµØ¯ Ú©Û’ Ù„ÛŒÛ’ ÛÛ’ Ø§ÙˆØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø´Ø§ÙˆØ±Øª Ù†ÛÛŒÚº ÛÛ’</i><br/>
            â€¢ <i>ÛÙ…ÛŒØ´Û Ø§Ù¾Ù†ÛŒ ØµÙˆØ±Øª Ø­Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛÙ„ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù¾ÛŒØ´Û ÙˆØ±Ø¤Úº Ø³Û’ Ù…Ø´Ø§ÙˆØ±Øª Ú©Ø±ÛŒÚº</i><br/>
            â€¢ <i>Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§ÙˆØ± Ø³Ø§Ø¨Ù‚Û’ ØªØ¨Ø¯ÛŒÙ„ ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚºØ› Ù…Ø§ÛØ±ÛŒÙ† Ø³Û’ Ù…ÙˆØ¬ÙˆØ¯Û Ø­ÛŒØ«ÛŒØª Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Ø±ÛŒÚº</i><br/>
            â€¢ <i>Ù„Ø§Ø¡ ÛŒØ§Ø± ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ù…Ø¯Ø¯ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ù„ÛŒÚ©Ù† Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø´Ø§ÙˆØ±Øª Ú©Ø§ Ù…ØªØ¨Ø§Ø¯Ù„ Ù†ÛÛŒÚº ÛÛ’</i>
            """
        elif detected_language == 'sd':
            methodology_text = """
            <b>ğŸ¤– AI Ø¬ÙŠ Ø·Ø§Ù‚Øª Ø³Ø§Ù† Ù‡Ù„Ù†Ø¯Ú™ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¬ÙŠ Ø·Ø±ÙŠÙ‚ÙŠ ÚªØ§Ø±:</b><br/><br/>
            Ù‡ÙŠ Ø±Ù¾ÙˆØ±Ù½ Ø¬Ø¯ÙŠØ¯ AI Ù½ÙŠÚªÙ†Ø§Ù„ÙˆØ¬ÙŠ Ø³Ø§Ù† Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³ Ø¬Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªÙ†Ø¯ÙŠ ØªÙŠØ§Ø± ÚªØ¦ÙŠ ÙˆØ¦ÙŠ Ø¢Ù‡ÙŠ:<br/>
            â€¢ <b>ÙˆÙŠÚªÙ½Ø± ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³ Ø¬ÙŠ Ú³ÙˆÙ„Ø§:</b> Ø§ÙŠÙ…Ø¨ÙŠÚŠÙ†Ú¯ Ø¬Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªÙ†Ø¯ÙŠ ÚªÙŠØ³ Ù„Ø§Ø¡ Ø¬Ùˆ Ø³ÙŠÙ…ÙŠÙ†Ù½Úª ØªØ¬Ø²ÙŠÙˆ<br/>
            â€¢ <b>LLM ØªØ¬Ø²ÙŠÙˆ:</b> ÙˆÚÙ† Ù»ÙˆÙ„ÙŠ Ù…Ø§ÚŠÙ„Ø² Ø¬Ùˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªÙ†Ø¯ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§ØµÙˆÙ„Ù† Ø¬Ùˆ ØªØ¬Ø²ÙŠÙˆ<br/>
            â€¢ <b>Ù…Ø§Ø®Ø° Ø¬ÙŠ ØªØµØ¯ÙŠÙ‚:</b> Ø³Ú€ Ø­ÙˆØ§Ù„Ø§ Ø³Ø±ÚªØ§Ø±ÙŠ Ø¹Ø¯Ø§Ù„Øª Ø¬ÙŠ Ø¯Ø³ØªØ§ÙˆÙŠØ² Ø³Ø§Ù† Ú³Ù†ÚÙŠÙ„ Ø¢Ù‡Ù†<br/>
            â€¢ <b>ÚªÙˆØ§Ù„Ù½ÙŠ Ø¬ÙŠ ÙŠÙ‚ÙŠÙ†ÙŠ ÚªØ±Ú»:</b> Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¯Ø±Ø³ØªÚ¯ÙŠ Ø¬ÙŠ ÚªÙŠØªØ±Ù† Ø¦ÙŠ Ù…Ø±Ø­Ù„Ù† Ø¬ÙŠ ØªØµØ¯ÙŠÙ‚<br/><br/>
            <b>âš ï¸ Ø§Ù‡Ù… Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„ Ø§Ø®ØªÙŠØ§Ø±ÙŠÙˆÙ†:</b><br/><br/>
            â€¢ <i>Ù‡ÙŠ Ø±Ù¾ÙˆØ±Ù½ ØµØ±Ù Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠ Ù…Ù‚ØµØ¯Ù† Ù„Ø§Ø¡Ù Ø¢Ù‡ÙŠ Û½ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØµÙ„Ø§Ø­ Ù†Ø§Ù‡ÙŠ</i><br/>
            â€¢ <i>Ù‡Ù…ÙŠØ´Ù‡ Ù¾Ù†Ù‡Ù† Ø¬ÙŠ ØµÙˆØ±ØªØ­Ø§Ù„ Ù„Ø§Ø¡Ù Ø§Ù‡Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„Ø² Ø³Ø§Ù† ØµÙ„Ø§Ø­ ÚªØ±ÙŠÙˆ</i><br/>
            â€¢ <i>Ù‚Ø§Ù†ÙˆÙ† Û½ Ø³Ø§Ø¨Ù‚Ø§ ØªØ¨Ø¯ÙŠÙ„ Ù¿ÙŠ Ø³Ú¯Ù‡Ù† Ù¿Ø§Ø› Ù…Ø§Ù‡Ø±Ù† Ø³Ø§Ù† Ù…ÙˆØ¬ÙˆØ¯Ù‡ Ø­ÙŠØ«ÙŠØª Ø¬ÙŠ ØªØµØ¯ÙŠÙ‚ ÚªØ±ÙŠÙˆ</i><br/>
            â€¢ <i>Ù„Ø§Ø¡ ÙŠØ§Ø± ØªØ­Ù‚ÙŠÙ‚ Ø¬ÙŠ Ù…Ø¯Ø¯ ÙØ±Ø§Ù‡Ù… ÚªØ±ÙŠ Ù¿Ùˆ Ù¾Ø± Ù¾Ø±ÙˆÙÙŠØ´Ù†Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØµÙ„Ø§Ø­ Ø¬Ùˆ Ù…ØªØ¨Ø§Ø¯Ù„ Ù†Ø§Ù‡ÙŠ</i>
            """
        elif detected_language == 'bl':
            methodology_text = """
            <b>ğŸ¤– AI Ú©ÛŒ Ø·Ø§Ù‚Øª Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø±:</b><br/><br/>
            ÛŒÛ Ø±Ù¾ÙˆØ±Ù¹ Ø¬Ø¯ÛŒØ¯ AI Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¬Ø§Ù…Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ ØªÛŒØ§Ø± Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’:<br/>
            â€¢ <b>ÙˆÛŒÚ©Ù¹Ø± ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³ Ú©ÛŒ ØªÙ„Ø§Ø´:</b> Ø§ÛŒÙ…Ø¨ÛŒÚˆÙ†Ú¯Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©ÛŒØ³ Ù„Ø§Ø¡ Ú©Ø§ Ø³ÛŒÙ…Ù†Ù¹Ú© ØªØ¬Ø²ÛŒÛ<br/>
            â€¢ <b>LLM ØªØ¬Ø²ÛŒÛ:</b> Ø¨Ú‘Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø§ØµÙˆÙ„ÙˆÚº Ú©Ø§ ØªØ¬Ø²ÛŒÛ<br/>
            â€¢ <b>Source Verification:</b> ØªÙ…Ø§Ù… Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø³Ø±Ú©Ø§Ø±ÛŒ Ø¹Ø¯Ø§Ù„Øª Ú©Û’ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ø³Û’ Ù…Ù†Ø³Ù„Ú© ÛÛŒÚº<br/>
            â€¢ <b>Ú©ÙˆØ§Ù„Ù¹ÛŒ Ú©ÛŒ ÛŒÙ‚ÛŒÙ† Ø¯ÛØ§Ù†ÛŒ:</b> Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©ÛŒ Ú©Ø«ÛŒØ± Ù…Ø±Ø­Ù„Û ØªØµØ¯ÛŒÙ‚<br/><br/>
            <b>âš ï¸ Ø§ÛÙ… Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ø§Ø®Ø·Ø§Ø±ÛŒÛ’:</b><br/><br/>
            â€¢ <i>ÛŒÛ Ø±Ù¾ÙˆØ±Ù¹ ØµØ±Ù Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÛŒ Ù…Ù‚Ø§ØµØ¯ Ú©Û’ Ù„ÛŒÛ’ ÛÛ’ Ø§ÙˆØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø´Ø§ÙˆØ±Øª Ù†ÛÛŒÚº ÛÛ’</i><br/>
            â€¢ <i>ÛÙ…ÛŒØ´Û Ø§Ù¾Ù†ÛŒ ØµÙˆØ±Øª Ø­Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛÙ„ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù¾ÛŒØ´Û ÙˆØ±Ø¤Úº Ø³Û’ Ù…Ø´Ø§ÙˆØ±Øª Ú©Ø±ÛŒÚº</i><br/>
            â€¢ <i>Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§ÙˆØ± Ø³Ø§Ø¨Ù‚Û’ ØªØ¨Ø¯ÛŒÙ„ ÛÙˆ Ø³Ú©ØªÛ’ ÛÛŒÚºØ› Ù…Ø§ÛØ±ÛŒÙ† Ø³Û’ Ù…ÙˆØ¬ÙˆØ¯Û Ø­ÛŒØ«ÛŒØª Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Ø±ÛŒÚº</i><br/>
            â€¢ <i>Ù„Ø§Ø¡ ÛŒØ§Ø± ØªØ­Ù‚ÛŒÙ‚ Ú©ÛŒ Ù…Ø¯Ø¯ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ù„ÛŒÚ©Ù† Ù¾ÛŒØ´Û ÙˆØ±Ø§Ù†Û Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø´Ø§ÙˆØ±Øª Ú©Ø§ Ù…ØªØ¨Ø§Ø¯Ù„ Ù†ÛÛŒÚº ÛÛ’</i>
            """
        else:
            methodology_text = """
            <b>ğŸ¤– AI-Powered Legal Research Methodology:</b><br/><br/>
            This report was generated using advanced AI technology combined with comprehensive legal databases:<br/>
            â€¢ <b>Vector Database Search:</b> Semantic analysis of case law using embeddings<br/>
            â€¢ <b>LLM Analysis:</b> Large language model synthesis of legal principles<br/>
            â€¢ <b>Source Verification:</b> All citations linked to official court documents<br/>
            â€¢ <b>Quality Assurance:</b> Multi-stage validation of legal accuracy<br/><br/>
            <b>âš ï¸ Important Professional Disclaimers:</b><br/><br/>
            â€¢ <i>This report is for informational purposes only and does not constitute legal advice</i><br/>
            â€¢ <i>Always consult qualified legal professionals for advice specific to your situation</i><br/>
            â€¢ <i>Laws and precedents may change; verify current status with legal experts</i><br/>
            â€¢ <i>LawYaar provides research assistance but is not a substitute for professional legal counsel</i>
            """

        story.append(Paragraph(methodology_text, styles['BodyText']))
        story.append(Spacer(1, 20))

        # ================================
        # FOOTER WITH CONTACT INFO (Localized)
        # ================================

        if detected_language == 'ur':
            footer_text = f"""
            <b>Ù„Ø§Ø¡ ÛŒØ§Ø± - AI Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Ø§ Ù…Ø¹Ø§ÙˆÙ†</b><br/>
            Ø¬Ø¯ÛŒØ¯ AI Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ø³Û’ Ú†Ù„ØªØ§ ÛÛ’ | Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©ÛŒ Ú©ÛŒØ³ Ù„Ø§Ø¡ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³<br/>
            ØªÛŒØ§Ø± Ú©ÛŒ Ú¯Ø¦ÛŒ: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}<br/>
            Ø±Ù¾ÙˆØ±Ù¹ Ø¢Ø¦ÛŒ ÚˆÛŒ: LY-{wa_id}-{datetime.now().strftime('%Y%m%d%H%M')}
            """
        elif detected_language == 'sd':
            footer_text = f"""
            <b>Ù„Ø§Ø¡ ÙŠØ§Ø± - AI Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø¬Ùˆ Ù…Ø¯Ø¯Ú¯Ø§Ø±</b><br/>
            Ø¬Ø¯ÙŠØ¯ AI Ù½ÙŠÚªÙ†Ø§Ù„ÙˆØ¬ÙŠ Ø³Ø§Ù† Ù‡Ù„Ù†Ø¯Ú™ | Ø³Ù¾Ø±ÙŠÙ… ÚªÙˆØ±Ù½ Ø¬ÙŠ ÚªÙŠØ³ Ù„Ø§Ø¡ ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³<br/>
            ØªÙŠØ§Ø± ÚªÙŠÙ„: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}<br/>
            Ø±Ù¾ÙˆØ±Ù½ Ø¢Ø¡Ù ÚŠÙŠ: LY-{wa_id}-{datetime.now().strftime('%Y%m%d%H%M')}
            """
        elif detected_language == 'bl':
            footer_text = f"""
            <b>Ù„Ø§Ø¡ ÛŒØ§Ø± - AI Ù‚Ø§Ù†ÙˆÙ†ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Ø§ Ù…Ø¹Ø§ÙˆÙ†</b><br/>
            Ø¬Ø¯ÛŒØ¯ AI Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ø³Û’ Ú†Ù„ØªØ§ ÛÛ’ | Ø³Ù¾Ø±ÛŒÙ… Ú©ÙˆØ±Ù¹ Ú©ÛŒ Ú©ÛŒØ³ Ù„Ø§Ø¡ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³<br/>
            ØªÛŒØ§Ø± Ú©ÛŒ Ú¯Ø¦ÛŒ: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}<br/>
            Ø±Ù¾ÙˆØ±Ù¹ Ø¢Ø¦ÛŒ ÚˆÛŒ: LY-{wa_id}-{datetime.now().strftime('%Y%m%d%H%M')}
            """
        else:
            footer_text = f"""
            <b>LawYaar - AI Legal Research Assistant</b><br/>
            Powered by Advanced AI Technology | Supreme Court Case Law Database<br/>
            Generated: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}<br/>
            Report ID: LY-{wa_id}-{datetime.now().strftime('%Y%m%d%H%M')}
            """

        footer_table_data = [[
            Paragraph(footer_text, styles['Footer'])
        ]]

        footer_table = Table(footer_table_data, colWidths=[7*inch])
        footer_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, -1), HexColor('#2C3E50')),
            ('TEXTCOLOR', (0, 0), (-1, -1), white),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('LEFTPADDING', (0, 0), (-1, -1), 10),
            ('RIGHTPADDING', (0, 0), (-1, -1), 10),
            ('TOPPADDING', (0, 0), (-1, -1), 8),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
        ]))

        story.append(footer_table)

        # Build PDF with error handling
        try:
            doc.build(story)
            logger.info(f"âœ… Enhanced PDF report generated successfully: {pdf_path}")
            return pdf_path
        except Exception as pdf_error:
            logger.error(f"âŒ Error building PDF: {pdf_error}", exc_info=True)

            # Try fallback with basic fonts
            try:
                logger.info("ğŸ”„ Attempting PDF generation with fallback fonts...")

                # Reset story with basic fonts
                story_fallback = []

                # Use only basic fonts for fallback
                fallback_styles = getSampleStyleSheet()

                # Simple title
                title_fallback = Paragraph("LawYaar Legal Research Report", fallback_styles['Title'])
                story_fallback.append(title_fallback)
                story_fallback.append(Spacer(1, 12))

                # Basic metadata
                meta_fallback = f"Generated for: {name}\nDate: {datetime.now().strftime('%B %d, %Y')}\nCases Analyzed: {doc_count}"
                story_fallback.append(Paragraph(meta_fallback, fallback_styles['Normal']))
                story_fallback.append(Spacer(1, 12))

                # Basic content
                if full_legal_response.strip():
                    content_fallback = escape(full_legal_response)[:2000]  # Limit content length
                    story_fallback.append(Paragraph(content_fallback, fallback_styles['Normal']))

                # Build with fallback
                doc_fallback = SimpleDocTemplate(pdf_path, pagesize=A4,
                                               rightMargin=72, leftMargin=72,
                                               topMargin=72, bottomMargin=18)
                doc_fallback.build(story_fallback)

                logger.info(f"âœ… PDF generated with fallback fonts: {pdf_path}")
                return pdf_path

            except Exception as fallback_error:
                logger.error(f"âŒ Fallback PDF generation also failed: {fallback_error}")
                return None

    except Exception as e:
        logger.error(f"âŒ Error generating enhanced PDF report: {e}", exc_info=True)
        return None


def _is_urdu_text(text: str) -> bool:
    """
    Check if text is already in Urdu.
    
    Args:
        text: Text to check
        
    Returns:
        bool: True if text is in Urdu, False otherwise
    """
    urdu_arabic_chars = sum(1 for char in text if '\u0600' <= char <= '\u06FF' or '\u0750' <= char <= '\u077F')
    return len(text) > 0 and urdu_arabic_chars > len(text) * 0.2


def _translate_to_urdu(english_text: str) -> str:
    """
    Translate English text to Urdu using Gemini API.
    
    Args:
        english_text: Text in English
        
    Returns:
        str: Translated text in Urdu
    """
    try:
        import google.generativeai as genai
        import os
        
        gemini_api_key = os.getenv('GEMINI_API_KEY')
        if not gemini_api_key:
            logger.error("GEMINI_API_KEY not found - cannot translate to Urdu")
            return english_text  # Return original if can't translate
        
        genai.configure(api_key=gemini_api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        translation_prompt = f"""Translate the following legal analysis from English to Urdu.
Maintain all legal terminology accuracy and preserve the structure (headings, bullet points, etc.).
Keep case citations and legal terms in English but translate explanations to Urdu.
Be professional and formal in Urdu.
Use proper Urdu script (Ø§Ø±Ø¯Ùˆ).

ENGLISH TEXT:
{english_text}

URDU TRANSLATION (Ø§Ø±Ø¯Ùˆ ØªØ±Ø¬Ù…Û):"""
        
        logger.info("Translating to Urdu with Gemini...")
        response = model.generate_content(translation_prompt)
        urdu_text = response.text.strip()
        
        logger.info(f"âœ… Translation successful")
        return urdu_text
        
    except Exception as e:
        logger.error(f"âŒ Translation error: {e}")
        return english_text  # Fallback to English if translation fails
